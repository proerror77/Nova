# Phase 1B ä»£ç æ¡†æ¶å’Œå®ç°æ¨¡æ¿

**æœ¬æ–‡æ¡£**: å³æ’å³ç”¨çš„ä»£ç éª¨æ¶
**ç›®æ ‡**: åŠ é€Ÿå¼€å‘ (å¤åˆ¶ â†’ ä¿®æ”¹ â†’ æµ‹è¯•)
**è¯­è¨€**: Rust (æ‰€æœ‰ä»£ç )

---

## ğŸ“¦ Task 1.1: Outbox æ¨¡å¼åº“

### æ–‡ä»¶: backend/libs/event-schema/src/outbox.rs

```rust
//! ç»Ÿä¸€çš„ Outbox äº‹ä»¶æ¨¡å‹å’Œæ“ä½œ

use uuid::Uuid;
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};
use sqlx::FromRow;

/// ç»Ÿä¸€çš„ Outbox äº‹ä»¶ç»“æ„
#[derive(Debug, Clone, FromRow, Serialize, Deserialize)]
pub struct OutboxEvent {
    /// äº‹ä»¶å”¯ä¸€ ID
    pub id: Uuid,

    /// ä¸šåŠ¡å¯¹è±¡ ID (message_id, post_id, user_id, etc)
    pub aggregate_id: Uuid,

    /// äº‹ä»¶ç±»å‹ (MessageCreated, PostLiked, FollowAdded, etc)
    pub event_type: String,

    /// äº‹ä»¶è´Ÿè½½ (JSON æ ¼å¼)
    pub payload: serde_json::Value,

    /// äº‹ä»¶ä¼˜å…ˆçº§ (0=Critical, 3=Low)
    pub priority: i32,

    /// åˆ›å»ºæ—¶é—´æˆ³
    pub created_at: DateTime<Utc>,

    /// å‘å¸ƒåˆ° Kafka çš„æ—¶é—´æˆ³ (NULL = æœªå‘å¸ƒ)
    pub published_at: Option<DateTime<Utc>>,

    /// é‡è¯•æ¬¡æ•°
    pub retry_count: i32,

    /// æœ€åä¸€æ¬¡é”™è¯¯ä¿¡æ¯
    pub last_error: Option<String>,
}

impl OutboxEvent {
    /// åˆ›å»ºæ–°çš„å¾…å‘å¸ƒäº‹ä»¶
    pub fn new(
        aggregate_id: Uuid,
        event_type: impl Into<String>,
        payload: serde_json::Value,
        priority: i32,
    ) -> Self {
        Self {
            id: Uuid::new_v4(),
            aggregate_id,
            event_type: event_type.into(),
            payload,
            priority,
            created_at: Utc::now(),
            published_at: None,
            retry_count: 0,
            last_error: None,
        }
    }

    /// ç”Ÿæˆ Kafka topic åç§°
    pub fn kafka_topic(&self) -> String {
        format!("nova_events_{}", self.event_type.to_lowercase())
    }

    /// ç”Ÿæˆ Kafka partition key (ç¡®ä¿é¡ºåº)
    pub fn partition_key(&self) -> String {
        self.aggregate_id.to_string()
    }

    /// è½¬æ¢ä¸º Kafka æ¶ˆæ¯
    pub fn to_kafka_message(&self) -> Result<Vec<u8>, serde_json::Error> {
        serde_json::to_vec(self)
    }
}

/// äº‹ä»¶ä¼˜å…ˆçº§å®šä¹‰
pub mod priority {
    pub const CRITICAL: i32 = 0;  // < 100ms å¤„ç† (ç›´æ’­å¼€å§‹ã€å®‰å…¨äº‹ä»¶)
    pub const HIGH: i32 = 1;      // < 1s å¤„ç† (æ¶ˆæ¯ã€è¯„è®º)
    pub const NORMAL: i32 = 2;    // < 5s å¤„ç† (èµã€å…³æ³¨)
    pub const LOW: i32 = 3;       // < 1min å¤„ç† (åˆ†æã€æ¸…ç†)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_create_outbox_event() {
        let aggregate_id = Uuid::new_v4();
        let payload = serde_json::json!({ "sender_id": "123" });
        let event = OutboxEvent::new(
            aggregate_id,
            "MessageCreated",
            payload,
            priority::HIGH,
        );

        assert_eq!(event.event_type, "MessageCreated");
        assert_eq!(event.priority, 1);
        assert_eq!(event.kafka_topic(), "nova_events_messagecreated");
        assert_eq!(event.partition_key(), aggregate_id.to_string());
    }
}
```

### æ–‡ä»¶: backend/libs/event-schema/src/lib.rs

```rust
//! Event schema library - æ‰€æœ‰æœåŠ¡å…±ç”¨çš„äº‹ä»¶å®šä¹‰

pub mod outbox;
pub mod events;

pub use outbox::{OutboxEvent, priority};
pub use events::DomainEvent;
```

### æ–‡ä»¶: backend/libs/event-schema/src/events.rs

```rust
//! é¢†åŸŸäº‹ä»¶å®šä¹‰ (ä¸šåŠ¡é©±åŠ¨)

use uuid::Uuid;
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};

/// æ‰€æœ‰é¢†åŸŸäº‹ä»¶çš„æšä¸¾
#[derive(Debug, Clone, Serialize, Deserialize)]
#[serde(tag = "event_type", content = "data")]
pub enum DomainEvent {
    // ===== Messaging Events =====
    MessageCreated {
        message_id: Uuid,
        sender_id: Uuid,
        recipient_id: Uuid,
        conversation_id: Uuid,
        content: String,
        created_at: DateTime<Utc>,
    },

    MessageEdited {
        message_id: Uuid,
        editor_id: Uuid,
        new_content: String,
        edited_at: DateTime<Utc>,
    },

    MessageDeleted {
        message_id: Uuid,
        deleter_id: Uuid,
        deleted_at: DateTime<Utc>,
    },

    // ===== Reaction Events =====
    ReactionAdded {
        reaction_id: Uuid,
        target_id: Uuid,  // message_id / post_id / comment_id
        target_type: String,  // "message" / "post" / "comment"
        user_id: Uuid,
        emoji: String,
        created_at: DateTime<Utc>,
    },

    ReactionRemoved {
        target_id: Uuid,
        target_type: String,
        user_id: Uuid,
        emoji: String,
        removed_at: DateTime<Utc>,
    },

    // ===== Follow Events =====
    FollowAdded {
        follower_id: Uuid,
        followee_id: Uuid,
        created_at: DateTime<Utc>,
    },

    FollowRemoved {
        follower_id: Uuid,
        followee_id: Uuid,
        removed_at: DateTime<Utc>,
    },

    // ===== Content Events =====
    PostCreated {
        post_id: Uuid,
        author_id: Uuid,
        title: String,
        content: String,
        tags: Vec<String>,
        created_at: DateTime<Utc>,
    },

    PostUpdated {
        post_id: Uuid,
        editor_id: Uuid,
        title: String,
        content: String,
        tags: Vec<String>,
        updated_at: DateTime<Utc>,
    },

    PostDeleted {
        post_id: Uuid,
        deleter_id: Uuid,
        deleted_at: DateTime<Utc>,
    },

    // ===== Notification Events (å‘å¸ƒç»™ç”¨æˆ·) =====
    NotificationCreated {
        notification_id: Uuid,
        user_id: Uuid,
        title: String,
        body: String,
        notification_type: String,  // "mention" / "follow" / "like" / etc
        data: serde_json::Value,
        created_at: DateTime<Utc>,
    },

    // ===== Search Index Events =====
    SearchIndexUpdated {
        document_id: Uuid,
        document_type: String,  // "post" / "user" / "comment"
        operation: String,  // "index" / "update" / "delete"
        updated_at: DateTime<Utc>,
    },

    // ===== Streaming Events =====
    StreamStarted {
        stream_id: Uuid,
        broadcaster_id: Uuid,
        title: String,
        started_at: DateTime<Utc>,
    },

    StreamEnded {
        stream_id: Uuid,
        broadcaster_id: Uuid,
        ended_at: DateTime<Utc>,
    },

    StreamMessagePosted {
        stream_id: Uuid,
        sender_id: Uuid,
        message: String,
        posted_at: DateTime<Utc>,
    },
}

impl DomainEvent {
    /// è·å–èšåˆæ ¹ ID (ç”¨ä½œ Kafka partition key)
    pub fn aggregate_id(&self) -> Uuid {
        match self {
            DomainEvent::MessageCreated { message_id, .. } => *message_id,
            DomainEvent::MessageEdited { message_id, .. } => *message_id,
            DomainEvent::MessageDeleted { message_id, .. } => *message_id,
            DomainEvent::ReactionAdded { target_id, .. } => *target_id,
            DomainEvent::ReactionRemoved { target_id, .. } => *target_id,
            DomainEvent::FollowAdded { follower_id, .. } => *follower_id,
            DomainEvent::FollowRemoved { follower_id, .. } => *follower_id,
            DomainEvent::PostCreated { post_id, .. } => *post_id,
            DomainEvent::PostUpdated { post_id, .. } => *post_id,
            DomainEvent::PostDeleted { post_id, .. } => *post_id,
            DomainEvent::NotificationCreated { notification_id, .. } => *notification_id,
            DomainEvent::SearchIndexUpdated { document_id, .. } => *document_id,
            DomainEvent::StreamStarted { stream_id, .. } => *stream_id,
            DomainEvent::StreamEnded { stream_id, .. } => *stream_id,
            DomainEvent::StreamMessagePosted { stream_id, .. } => *stream_id,
        }
    }

    /// è·å–äº‹ä»¶ç±»å‹å­—ç¬¦ä¸²
    pub fn event_type(&self) -> String {
        match self {
            DomainEvent::MessageCreated { .. } => "MessageCreated".to_string(),
            DomainEvent::MessageEdited { .. } => "MessageEdited".to_string(),
            DomainEvent::MessageDeleted { .. } => "MessageDeleted".to_string(),
            DomainEvent::ReactionAdded { .. } => "ReactionAdded".to_string(),
            DomainEvent::ReactionRemoved { .. } => "ReactionRemoved".to_string(),
            DomainEvent::FollowAdded { .. } => "FollowAdded".to_string(),
            DomainEvent::FollowRemoved { .. } => "FollowRemoved".to_string(),
            DomainEvent::PostCreated { .. } => "PostCreated".to_string(),
            DomainEvent::PostUpdated { .. } => "PostUpdated".to_string(),
            DomainEvent::PostDeleted { .. } => "PostDeleted".to_string(),
            DomainEvent::NotificationCreated { .. } => "NotificationCreated".to_string(),
            DomainEvent::SearchIndexUpdated { .. } => "SearchIndexUpdated".to_string(),
            DomainEvent::StreamStarted { .. } => "StreamStarted".to_string(),
            DomainEvent::StreamEnded { .. } => "StreamEnded".to_string(),
            DomainEvent::StreamMessagePosted { .. } => "StreamMessagePosted".to_string(),
        }
    }

    /// è·å–äº‹ä»¶ä¼˜å…ˆçº§
    pub fn priority(&self) -> i32 {
        use crate::priority::*;

        match self {
            // Critical (P0): ç³»ç»Ÿå…³é”®äº‹ä»¶
            DomainEvent::StreamStarted { .. } => CRITICAL,
            DomainEvent::StreamEnded { .. } => CRITICAL,

            // High (P1): ç”¨æˆ·äº¤äº’
            DomainEvent::MessageCreated { .. } => HIGH,
            DomainEvent::MessageEdited { .. } => HIGH,
            DomainEvent::PostCreated { .. } => HIGH,
            DomainEvent::NotificationCreated { .. } => HIGH,

            // Normal (P2): ç¤¾äº¤ä¿¡å·
            DomainEvent::ReactionAdded { .. } => NORMAL,
            DomainEvent::FollowAdded { .. } => NORMAL,

            // Low (P3): ç´¢å¼•å’Œåˆ†æ
            DomainEvent::SearchIndexUpdated { .. } => LOW,
            DomainEvent::MessageDeleted { .. } => LOW,
            DomainEvent::ReactionRemoved { .. } => LOW,
            DomainEvent::FollowRemoved { .. } => LOW,
            DomainEvent::PostUpdated { .. } => LOW,
            DomainEvent::PostDeleted { .. } => LOW,
            DomainEvent::StreamMessagePosted { .. } => NORMAL,
        }
    }
}
```

---

## ğŸš€ Task 1.2: events-service æ ¸å¿ƒå®ç°

### æ–‡ä»¶: backend/events-service/src/services/outbox.rs

```rust
//! Outbox å‘å¸ƒå™¨ - æ‰«æå¾…å‘å¸ƒäº‹ä»¶å¹¶æ¨é€åˆ° Kafka

use uuid::Uuid;
use sqlx::PgPool;
use rdkafka::producer::FutureProducer;
use rdkafka::message::FutureRecord;
use std::time::Duration;
use tracing::{debug, error, info};
use nova_event_schema::OutboxEvent;

pub struct OutboxPublisher {
    db: PgPool,
    kafka_producer: FutureProducer,
    batch_size: i32,
    flush_interval_ms: u64,
}

impl OutboxPublisher {
    pub fn new(
        db: PgPool,
        kafka_producer: FutureProducer,
        batch_size: i32,
        flush_interval_ms: u64,
    ) -> Self {
        Self {
            db,
            kafka_producer,
            batch_size,
            flush_interval_ms,
        }
    }

    /// å¯åŠ¨åå°å‘å¸ƒä»»åŠ¡ (æ°¸ä¹…è¿è¡Œ)
    pub async fn start(self) {
        let mut ticker = tokio::time::interval(
            Duration::from_millis(self.flush_interval_ms)
        );

        loop {
            ticker.tick().await;

            match self.publish_batch().await {
                Ok(count) => {
                    if count > 0 {
                        debug!("Published {} outbox events", count);
                    }
                }
                Err(e) => {
                    error!("Failed to publish outbox batch: {}", e);
                    // ç»§ç»­è¿è¡Œï¼Œä¸å´©æºƒ
                }
            }
        }
    }

    /// å‘å¸ƒä¸€æ‰¹å¾…å‘å¸ƒçš„äº‹ä»¶
    async fn publish_batch(&self) -> Result<usize, Box<dyn std::error::Error>> {
        // 1. æŸ¥è¯¢æœªå‘å¸ƒçš„äº‹ä»¶
        let events: Vec<OutboxEvent> = sqlx::query_as(
            "SELECT id, aggregate_id, event_type, payload, priority, \
                    created_at, published_at, retry_count, last_error \
             FROM outbox_events \
             WHERE published_at IS NULL \
             ORDER BY priority ASC, created_at ASC \
             LIMIT $1"
        )
        .bind(self.batch_size)
        .fetch_all(&self.db)
        .await?;

        if events.is_empty() {
            return Ok(0);
        }

        info!("Publishing {} events to Kafka", events.len());

        // 2. æ‰¹é‡å‘é€åˆ° Kafka
        let mut send_futures = Vec::new();

        for event in &events {
            let topic = event.kafka_topic();
            let key = event.partition_key();
            let payload = serde_json::to_vec(&event)?;

            let record = FutureRecord::to(&topic)
                .key(&key)
                .payload(&payload)
                .timestamp(event.created_at.timestamp_millis());

            let future = self.kafka_producer.send(
                record,
                Duration::from_secs(5),
            );

            send_futures.push((event.id, future));
        }

        // 3. ç­‰å¾…æ‰€æœ‰å‘é€å®Œæˆ
        let mut success_count = 0;

        for (event_id, future) in send_futures {
            match future.await {
                Ok(_) => {
                    // æ›´æ–°å‘å¸ƒæ—¶é—´æˆ³
                    sqlx::query(
                        "UPDATE outbox_events \
                         SET published_at = NOW() \
                         WHERE id = $1"
                    )
                    .bind(event_id)
                    .execute(&self.db)
                    .await?;

                    success_count += 1;
                }
                Err((e, _)) => {
                    // æ›´æ–°é‡è¯•è®¡æ•°
                    sqlx::query(
                        "UPDATE outbox_events \
                         SET retry_count = retry_count + 1, \
                             last_error = $1 \
                         WHERE id = $2"
                    )
                    .bind(format!("Kafka error: {}", e))
                    .bind(event_id)
                    .execute(&self.db)
                    .await?;

                    error!("Failed to publish event {}: {}", event_id, e);
                }
            }
        }

        Ok(success_count)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    // æ³¨: é›†æˆæµ‹è¯•éœ€è¦è¿è¡Œ PostgreSQL å’Œ Kafka
    // è§ backend/tests/outbox_publisher_test.rs
}
```

### æ–‡ä»¶: backend/events-service/src/grpc/mod.rs (ç‰‡æ®µ)

```rust
//! gRPC æœåŠ¡å®ç°

use tonic::{Request, Response, Status};
use uuid::Uuid;
use nova_events::events_service_server::EventsService;
use nova_events::{PublishEventRequest, PublishEventResponse};

pub struct EventsServiceImpl {
    db: sqlx::PgPool,
    // ... å…¶ä»–å­—æ®µ
}

#[tonic::async_trait]
impl EventsService for EventsServiceImpl {
    async fn publish_event(
        &self,
        request: Request<PublishEventRequest>,
    ) -> Result<Response<PublishEventResponse>, Status> {
        let req = request.into_inner();

        // 1. åŸºæœ¬éªŒè¯
        if req.event_type.is_empty() {
            return Err(Status::invalid_argument("event_type is required"));
        }

        // 2. éªŒè¯äº‹ä»¶ schema (å¯é€‰ï¼Œä½†æ¨è)
        if !self.schema_registry.is_valid(&req.event_type, &req.payload) {
            return Err(Status::invalid_argument(
                format!("Invalid payload for event type: {}", req.event_type)
            ));
        }

        // 3. ä¿å­˜åˆ° Outbox è¡¨
        let event_id = Uuid::new_v4();
        let aggregate_id = Uuid::parse_str(&req.aggregate_id)
            .map_err(|_| Status::invalid_argument("Invalid aggregate_id"))?;

        sqlx::query(
            "INSERT INTO outbox_events \
             (id, aggregate_id, event_type, payload, priority, created_at) \
             VALUES ($1, $2, $3, $4, $5, NOW())"
        )
        .bind(event_id)
        .bind(aggregate_id)
        .bind(&req.event_type)
        .bind(&req.payload)
        .bind(req.priority as i32)
        .execute(&self.db)
        .await
        .map_err(|e| Status::internal(format!("Failed to save event: {}", e)))?;

        Ok(Response::new(PublishEventResponse {
            event_id: event_id.to_string(),
            status: "QUEUED".to_string(),  // ç­‰å¾… Outbox Publisher å‘å¸ƒ
        }))
    }
}
```

---

## ğŸ—„ï¸ æ•°æ®åº“è¿ç§»

### æ–‡ä»¶: backend/events-service/src/db/migrations/001_create_outbox_tables.sql

```sql
-- ===== Outbox Events Table (Core) =====
CREATE TABLE IF NOT EXISTS outbox_events (
    -- ä¸»é”®å’Œå…³è”
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    aggregate_id UUID NOT NULL,

    -- äº‹ä»¶ç±»å‹å’Œè´Ÿè½½
    event_type VARCHAR(255) NOT NULL,
    payload JSONB NOT NULL,

    -- å…ƒæ•°æ®
    priority SMALLINT NOT NULL DEFAULT 2,  -- 0=Critical, 1=High, 2=Normal, 3=Low
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    published_at TIMESTAMPTZ,  -- NULL = æœªå‘å¸ƒ

    -- é‡è¯•å’Œé”™è¯¯å¤„ç†
    retry_count INT NOT NULL DEFAULT 0,
    last_error TEXT,

    -- ç´¢å¼•ä¼˜åŒ–
    CONSTRAINT check_priority CHECK (priority >= 0 AND priority <= 3),
    CONSTRAINT check_retry_count CHECK (retry_count >= 0)
);

-- ç´¢å¼• 1: å¿«é€Ÿå®šä½å¾…å‘å¸ƒäº‹ä»¶
CREATE INDEX idx_outbox_unpublished
ON outbox_events(priority ASC, created_at ASC)
WHERE published_at IS NULL;

-- ç´¢å¼• 2: æŒ‰èšåˆæ ¹æŸ¥è¯¢ (ç”¨äºé‡æ”¾)
CREATE INDEX idx_outbox_aggregate
ON outbox_events(aggregate_id, event_type);

-- ç´¢å¼• 3: æŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢
CREATE INDEX idx_outbox_created
ON outbox_events(created_at DESC);

-- ç´¢å¼• 4: ç›‘æ§å’Œå‘Šè­¦
CREATE INDEX idx_outbox_failed
ON outbox_events(retry_count DESC)
WHERE published_at IS NULL AND retry_count > 0;


-- ===== Event Schema Registry =====
CREATE TABLE IF NOT EXISTS event_schemas (
    event_type VARCHAR(255) NOT NULL,
    schema_version INT NOT NULL,
    schema_definition JSONB NOT NULL,  -- JSON Schema
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    PRIMARY KEY (event_type, schema_version),
    CONSTRAINT check_version CHECK (schema_version > 0)
);

-- åˆ›å»ºåˆå§‹ç‰ˆæœ¬
INSERT INTO event_schemas (event_type, schema_version, schema_definition)
VALUES (
    'MessageCreated',
    1,
    '{
        "type": "object",
        "properties": {
            "message_id": {"type": "string", "format": "uuid"},
            "sender_id": {"type": "string", "format": "uuid"},
            "recipient_id": {"type": "string", "format": "uuid"},
            "content": {"type": "string"}
        },
        "required": ["message_id", "sender_id", "recipient_id", "content"]
    }'::jsonb
);


-- ===== Kafka Topic Metadata =====
CREATE TABLE IF NOT EXISTS kafka_topics (
    topic_name VARCHAR(255) PRIMARY KEY,
    event_type VARCHAR(255) NOT NULL,
    partition_count INT NOT NULL DEFAULT 3,
    replication_factor INT NOT NULL DEFAULT 2,
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),

    UNIQUE(event_type),
    CONSTRAINT check_partitions CHECK (partition_count > 0),
    CONSTRAINT check_replication CHECK (replication_factor > 0)
);

-- åˆ›å»ºåˆå§‹ topics
INSERT INTO kafka_topics (topic_name, event_type, partition_count)
VALUES
    ('nova_events_messagecreated', 'MessageCreated', 3),
    ('nova_events_reactionadded', 'ReactionAdded', 3),
    ('nova_events_followadded', 'FollowAdded', 3),
    ('nova_events_postcreated', 'PostCreated', 3)
ON CONFLICT DO NOTHING;
```

---

## âœ… å•å…ƒæµ‹è¯•æ¨¡æ¿

### æ–‡ä»¶: backend/events-service/src/services/outbox_test.rs

```rust
#[cfg(test)]
mod tests {
    use super::*;
    use sqlx::postgres::PgPoolOptions;
    use testcontainers::clients::Cli;
    use testcontainers::images::postgres::Postgres;

    async fn setup_db() -> PgPool {
        let docker = Cli::default();
        let postgres = docker.run(Postgres::default());

        let connection_string = format!(
            "postgresql://postgres:postgres@{}",
            postgres.get_host_port_ipv4(5432)
        );

        let pool = PgPoolOptions::new()
            .connect(&connection_string)
            .await
            .unwrap();

        // è¿è¡Œè¿ç§»
        sqlx::raw_sql(MIGRATIONS)
            .execute(&pool)
            .await
            .unwrap();

        pool
    }

    #[tokio::test]
    async fn test_outbox_event_persistence() {
        let db = setup_db().await;
        let event_id = Uuid::new_v4();
        let aggregate_id = Uuid::new_v4();

        sqlx::query(
            "INSERT INTO outbox_events \
             (id, aggregate_id, event_type, payload, created_at) \
             VALUES ($1, $2, $3, $4, NOW())"
        )
        .bind(event_id)
        .bind(aggregate_id)
        .bind("MessageCreated")
        .bind(serde_json::json!({"test": "data"}))
        .execute(&db)
        .await
        .unwrap();

        // éªŒè¯æ’å…¥
        let row: (Uuid, bool) = sqlx::query_as(
            "SELECT id, published_at IS NULL FROM outbox_events WHERE id = $1"
        )
        .bind(event_id)
        .fetch_one(&db)
        .await
        .unwrap();

        assert_eq!(row.0, event_id);
        assert!(row.1);  // published_at IS NULL
    }

    #[tokio::test]
    async fn test_outbox_query_performance() {
        // æ’å…¥ 10000 ä¸ªäº‹ä»¶ï¼ŒéªŒè¯æŸ¥è¯¢é€Ÿåº¦ < 100ms
        // ...
    }
}
```

---

## ğŸ¬ ä¸‹ä¸€æ­¥è¡ŒåŠ¨

1. **ç«‹å³å¤åˆ¶ä»£ç **:
   ```bash
   # 1. åˆ›å»ºæ–‡ä»¶
   touch backend/libs/event-schema/src/outbox.rs
   touch backend/libs/event-schema/src/events.rs
   touch backend/events-service/src/services/outbox.rs

   # 2. å¤åˆ¶ä¸Šé¢çš„å†…å®¹
   # 3. ä¿®æ”¹ Cargo.toml ä¾èµ–
   # 4. è¿è¡Œ cargo build
   ```

2. **è¿è¡Œæ•°æ®åº“è¿ç§»**:
   ```bash
   sqlx migrate run --database-url postgresql://...
   ```

3. **å¯åŠ¨æœ¬åœ°æµ‹è¯•**:
   ```bash
   cargo test --package events-service
   ```

4. **æ€§èƒ½åŸºå‡†**:
   ```bash
   # æµ‹è¯• Outbox å‘å¸ƒå»¶è¿Ÿ
   time cargo run --release --bin events-service
   ```

---

**é¢„è®¡å®Œæˆ**:
- Task 1.1 (Outbox): 16h
- Task 1.2 (events-service): 32h

å‡†å¤‡å¼€å§‹? ğŸš€

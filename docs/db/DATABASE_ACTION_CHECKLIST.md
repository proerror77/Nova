# Nova æ•°æ®åº“é‡æ„è¡ŒåŠ¨æ¸…å•

**æ—¥æœŸ**: 2025-11-11
**ä¼˜å…ˆçº§**: ğŸ”´ CRITICAL
**å·¥ä½œé‡**: 8 å‘¨ (2 Backend Engineers + 1 DevOps Engineer)

---

## ğŸš¨ ç«‹å³è¡ŒåŠ¨ (æœ¬å‘¨å®Œæˆ)

### [ ] 1. è·å¾—ç®¡ç†å±‚æ‰¹å‡†

**è´Ÿè´£äºº**: é¡¹ç›®ç»ç†
**æ—¶é—´**: 1-2 å¤©

**éœ€è¦æ‰¹å‡†çš„å†…å®¹**:
- [ ] æˆæœ¬å¢åŠ é¢„ç®—: $1000/æœˆ (æœ€ç»ˆä¼˜åŒ–å $653/æœˆ)
- [ ] å·¥ç¨‹èµ„æºåˆ†é…: 2 Backend + 1 DevOps (8 å‘¨å…¨èŒ)
- [ ] é£é™©æ¥å—: æ•°æ®åº“è¿ç§»çš„ä¸­ç­‰é£é™©
- [ ] æ—¶é—´æ‰¿è¯º: 8 å‘¨å®Œæˆé‡æ„

**æäº¤ææ–™**:
- [x] [DATABASE_EXECUTIVE_SUMMARY.md](DATABASE_EXECUTIVE_SUMMARY.md)
- [x] [DATABASE_ARCHITECTURE_ANALYSIS.md](DATABASE_ARCHITECTURE_ANALYSIS.md)
- [ ] ROI åˆ†ææŠ¥å‘Š (å¾…è¡¥å……)

---

### [ ] 2. æ•°æ®ä¸€è‡´æ€§éªŒè¯æµ‹è¯•

**è´Ÿè´£äºº**: Backend Team Lead
**æ—¶é—´**: 2-3 å¤©

#### [ ] 2.1 éªŒè¯ `users` è¡¨æ•°æ®å·®å¼‚

```sql
-- è¿æ¥åˆ°æ•°æ®åº“
kubectl exec -n nova postgres-7fd85d47f6-57ddz -- psql -U postgres

-- æ£€æŸ¥è¡Œæ•°å·®å¼‚
SELECT 'nova_auth' AS db, COUNT(*) FROM nova_auth.users
UNION ALL
SELECT 'nova_staging' AS db, COUNT(*) FROM nova_staging.users;

-- æŸ¥æ‰¾ä¸ä¸€è‡´çš„è®°å½• (æŒ‰ ID)
SELECT
  a.id,
  a.username AS auth_username,
  s.username AS staging_username,
  a.email AS auth_email,
  s.email AS staging_email,
  a.updated_at AS auth_updated,
  s.updated_at AS staging_updated
FROM nova_auth.users a
LEFT JOIN nova_staging.users s ON a.id = s.id
WHERE
  a.username != s.username
  OR a.email != s.email
  OR a.display_name != s.display_name;

-- æŸ¥æ‰¾å­¤å„¿è®°å½• (åœ¨ auth ä½†ä¸åœ¨ staging)
SELECT id, username, email, created_at
FROM nova_auth.users
WHERE id NOT IN (SELECT id FROM nova_staging.users);

-- æŸ¥æ‰¾å­¤å„¿è®°å½• (åœ¨ staging ä½†ä¸åœ¨ auth)
SELECT id, username, email, created_at
FROM nova_staging.users
WHERE id NOT IN (SELECT id FROM nova_auth.users);
```

**è¾“å‡ºç»“æœåˆ°æ–‡ä»¶**:
```bash
kubectl exec -n nova postgres-7fd85d47f6-57ddz -- psql -U postgres -d nova_auth -c "
SELECT ... (ä¸Šè¿°æŸ¥è¯¢)
" > /tmp/user_table_inconsistency_report.txt
```

**æœŸæœ›ç»“æœ**:
- è®°å½•ä¸ä¸€è‡´ç‡ (ç›®æ ‡: < 1%)
- å­¤å„¿è®°å½•æ•°é‡
- æœ€åæ›´æ–°æ—¶é—´å·®å¼‚

---

#### [ ] 2.2 æµ‹è¯•ç”¨æˆ·åˆ é™¤åœºæ™¯

**æµ‹è¯•è„šæœ¬**: `backend/scripts/test_user_deletion.sh`

```bash
#!/bin/bash
set -e

echo "=== ç”¨æˆ·åˆ é™¤åœºæ™¯æµ‹è¯• ==="

# 1. åˆ›å»ºæµ‹è¯•ç”¨æˆ·
USER_ID=$(uuidgen)
echo "åˆ›å»ºæµ‹è¯•ç”¨æˆ·: $USER_ID"

kubectl exec -n nova postgres-7fd85d47f6-57ddz -- psql -U postgres -d nova_auth -c "
INSERT INTO users (id, username, email, password_hash)
VALUES ('$USER_ID', 'test_delete_user', 'test@delete.com', 'hash');
"

kubectl exec -n nova postgres-7fd85d47f6-57ddz -- psql -U postgres -d nova_staging -c "
INSERT INTO users (id, username, email, password_hash)
VALUES ('$USER_ID', 'test_delete_user', 'test@delete.com', 'hash');

INSERT INTO user_profiles (id, username, email)
VALUES ('$USER_ID', 'test_delete_user', 'test@delete.com');

INSERT INTO search_history (id, user_id, query_type, query_text)
VALUES (gen_random_uuid(), '$USER_ID', 'user', 'test query');

INSERT INTO activity_logs (id, user_id, activity_type)
VALUES (gen_random_uuid(), '$USER_ID', 'test_activity');
"

# 2. éªŒè¯æ•°æ®å­˜åœ¨
echo "éªŒè¯æ•°æ®åˆ›å»ºæˆåŠŸ..."
kubectl exec -n nova postgres-7fd85d47f6-57ddz -- psql -U postgres -d nova_staging -c "
SELECT 'user_profiles' AS table, COUNT(*) FROM user_profiles WHERE id = '$USER_ID'
UNION ALL
SELECT 'search_history', COUNT(*) FROM search_history WHERE user_id = '$USER_ID'
UNION ALL
SELECT 'activity_logs', COUNT(*) FROM activity_logs WHERE user_id = '$USER_ID';
"

# 3. åˆ é™¤ staging ç”¨æˆ· (è§¦å‘ CASCADE)
echo "åˆ é™¤ staging.users (æµ‹è¯• CASCADE è¡Œä¸º)..."
kubectl exec -n nova postgres-7fd85d47f6-57ddz -- psql -U postgres -d nova_staging -c "
DELETE FROM users WHERE id = '$USER_ID';
"

# 4. æ£€æŸ¥çº§è”åˆ é™¤ç»“æœ
echo "æ£€æŸ¥çº§è”åˆ é™¤ç»“æœ..."
kubectl exec -n nova postgres-7fd85d47f6-57ddz -- psql -U postgres -d nova_staging -c "
SELECT 'user_profiles' AS table, COUNT(*) FROM user_profiles WHERE id = '$USER_ID'
UNION ALL
SELECT 'search_history', COUNT(*) FROM search_history WHERE user_id = '$USER_ID'
UNION ALL
SELECT 'activity_logs', COUNT(*) FROM activity_logs WHERE user_id = '$USER_ID';
"

# 5. æ£€æŸ¥ auth è¡¨æ˜¯å¦ä»ç„¶å­˜åœ¨
echo "æ£€æŸ¥ nova_auth.users æ˜¯å¦ä»å­˜åœ¨..."
kubectl exec -n nova postgres-7fd85d47f6-57ddz -- psql -U postgres -d nova_auth -c "
SELECT id, username, email FROM users WHERE id = '$USER_ID';
"

echo "=== æµ‹è¯•å®Œæˆ ==="
```

**æ‰§è¡Œæµ‹è¯•**:
```bash
chmod +x backend/scripts/test_user_deletion.sh
./backend/scripts/test_user_deletion.sh > /tmp/user_deletion_test_result.txt
```

**åˆ†æç»“æœ**:
- [ ] CASCADE åˆ é™¤æ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œ?
- [ ] `nova_auth.users` æ˜¯å¦æ®‹ç•™?
- [ ] æ˜¯å¦äº§ç”Ÿå­¤å„¿è®°å½•?

---

#### [ ] 2.3 æŸ¥æ‰¾ç°æœ‰å­¤å„¿è®°å½•

```sql
-- å­¤å„¿ user_profiles (user å·²åˆ é™¤ä½† profile ä»å­˜åœ¨)
SELECT COUNT(*) AS orphan_profiles
FROM nova_staging.user_profiles p
WHERE NOT EXISTS (
  SELECT 1 FROM nova_staging.users u WHERE u.id = p.id
);

-- å­¤å„¿ search_history
SELECT COUNT(*) AS orphan_search_history
FROM nova_staging.search_history h
WHERE NOT EXISTS (
  SELECT 1 FROM nova_staging.users u WHERE u.id = h.user_id
);

-- å­¤å„¿ activity_logs
SELECT COUNT(*) AS orphan_activity_logs
FROM nova_staging.activity_logs l
WHERE NOT EXISTS (
  SELECT 1 FROM nova_staging.users u WHERE u.id = l.user_id
);

-- å­¤å„¿ reports
SELECT COUNT(*) AS orphan_reports
FROM nova_staging.reports r
WHERE
  NOT EXISTS (SELECT 1 FROM nova_staging.users u WHERE u.id = r.reporter_id)
  OR NOT EXISTS (SELECT 1 FROM nova_staging.users u WHERE u.id = r.reported_user_id);
```

**è¾“å‡ºæŠ¥å‘Š**:
```bash
kubectl exec -n nova postgres-7fd85d47f6-57ddz -- psql -U postgres -d nova_staging -c "
$(cat backend/scripts/find_orphan_records.sql)
" > /tmp/orphan_records_report.txt
```

---

### [ ] 3. æŠ€æœ¯æ–¹æ¡ˆè¯„å®¡ä¼šè®®

**è´Ÿè´£äºº**: Technical Lead
**æ—¶é—´**: åŠå¤© (4 å°æ—¶)

#### è®®ç¨‹

**09:00-10:00 - é—®é¢˜åˆ†æ**
- [ ] å±•ç¤ºæ•°æ®ä¸€è‡´æ€§æµ‹è¯•ç»“æœ
- [ ] è®¨è®ºè·¨æœåŠ¡å¤–é”®çš„å½±å“
- [ ] è¯„ä¼°å½“å‰æ¶æ„é£é™©

**10:00-11:00 - è§£å†³æ–¹æ¡ˆè®¾è®¡**
- [ ] auth-service gRPC API è®¾è®¡
  - GetUser(user_id) â†’ UserInfo
  - CheckUserExists(user_id) â†’ bool
  - GetUserBatch(user_ids[]) â†’ UserInfo[]
- [ ] äº‹ä»¶å®šä¹‰
  - UserCreated
  - UserUpdated
  - UserDeleted
- [ ] ç¼“å­˜ç­–ç•¥ (Redis + æœ¬åœ°ç¼“å­˜)

**11:00-12:00 - è¿ç§»ç­–ç•¥**
- [ ] Expand-Contract æ¨¡å¼ç»†èŠ‚
- [ ] åŒå†™æœŸé—´æ•°æ®ä¸€è‡´æ€§ä¿è¯
- [ ] å›æ»šè®¡åˆ’

**13:00-14:00 - æµ‹è¯•ç­–ç•¥**
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ç›®æ ‡ (80%+)
- [ ] é›†æˆæµ‹è¯•åœºæ™¯
- [ ] è´Ÿè½½æµ‹è¯• (k6)
- [ ] æ•°æ®å¯¹è´¦è„šæœ¬

**è¾“å‡ºç‰©**:
- [ ] æŠ€æœ¯è®¾è®¡æ–‡æ¡£ (TDD)
- [ ] API è§„èŒƒ (Protobuf)
- [ ] è¿ç§»æ—¶é—´è¡¨ (è¯¦ç»†åˆ°å¤©)
- [ ] é£é™©ç¼“è§£çŸ©é˜µ

---

## ğŸ“‹ Week 1-2: æ¶ˆé™¤ `users` è¡¨é‡å¤

### [ ] Week 1: auth-service API å¼€å‘

**è´Ÿè´£äºº**: Backend Engineer #1

#### [ ] 1.1 å®šä¹‰ Protobuf API

**æ–‡ä»¶**: `backend/proto/auth_service.proto`

```protobuf
syntax = "proto3";

package nova.auth.v1;

import "google/protobuf/timestamp.proto";

service AuthService {
  // è·å–å•ä¸ªç”¨æˆ·ä¿¡æ¯
  rpc GetUser(GetUserRequest) returns (GetUserResponse);

  // æ‰¹é‡è·å–ç”¨æˆ·ä¿¡æ¯
  rpc GetUserBatch(GetUserBatchRequest) returns (GetUserBatchResponse);

  // æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å­˜åœ¨
  rpc CheckUserExists(CheckUserExistsRequest) returns (CheckUserExistsResponse);

  // éªŒè¯ JWT Token
  rpc ValidateToken(ValidateTokenRequest) returns (ValidateTokenResponse);
}

message GetUserRequest {
  string user_id = 1;  // UUID
}

message GetUserResponse {
  User user = 1;
}

message GetUserBatchRequest {
  repeated string user_ids = 1;  // UUID[]
}

message GetUserBatchResponse {
  repeated User users = 1;
}

message CheckUserExistsRequest {
  string user_id = 1;  // UUID
}

message CheckUserExistsResponse {
  bool exists = 1;
}

message User {
  string id = 1;
  string username = 2;
  string email = 3;
  string display_name = 4;
  string avatar_url = 5;
  bool is_active = 6;
  bool email_verified = 7;
  google.protobuf.Timestamp created_at = 8;
  google.protobuf.Timestamp updated_at = 9;
}
```

**ä»»åŠ¡**:
- [ ] å®šä¹‰ Protobuf æ–‡ä»¶
- [ ] ç”Ÿæˆ Rust ä»£ç : `cargo build -p proto`
- [ ] ä»£ç å®¡æŸ¥: Backend Team

---

#### [ ] 1.2 å®ç° auth-service gRPC Server

**æ–‡ä»¶**: `backend/auth-service/src/grpc/user_service.rs`

```rust
use tonic::{Request, Response, Status};
use proto::auth_service_server::AuthService;
use proto::{GetUserRequest, GetUserResponse, User};

pub struct AuthServiceImpl {
    db_pool: PgPool,
    cache: Arc<RedisPool>,
}

#[tonic::async_trait]
impl AuthService for AuthServiceImpl {
    async fn get_user(
        &self,
        request: Request<GetUserRequest>,
    ) -> Result<Response<GetUserResponse>, Status> {
        let user_id = Uuid::parse_str(&request.into_inner().user_id)
            .map_err(|_| Status::invalid_argument("Invalid user_id"))?;

        // 1. å°è¯•ä» Redis ç¼“å­˜è·å–
        if let Some(cached_user) = self.get_from_cache(user_id).await? {
            return Ok(Response::new(GetUserResponse {
                user: Some(cached_user),
            }));
        }

        // 2. ä»æ•°æ®åº“æŸ¥è¯¢
        let user = sqlx::query_as!(
            UserModel,
            r#"
            SELECT id, username, email, display_name, avatar_url,
                   is_active, email_verified, created_at, updated_at
            FROM users
            WHERE id = $1 AND deleted_at IS NULL
            "#,
            user_id
        )
        .fetch_optional(&self.db_pool)
        .await
        .map_err(|e| Status::internal(format!("Database error: {}", e)))?
        .ok_or_else(|| Status::not_found("User not found"))?;

        // 3. å†™å…¥ Redis ç¼“å­˜
        self.cache_user(&user).await?;

        Ok(Response::new(GetUserResponse {
            user: Some(user.into()),
        }))
    }

    async fn get_user_batch(
        &self,
        request: Request<GetUserBatchRequest>,
    ) -> Result<Response<GetUserBatchResponse>, Status> {
        let user_ids: Vec<Uuid> = request
            .into_inner()
            .user_ids
            .iter()
            .filter_map(|id| Uuid::parse_str(id).ok())
            .collect();

        if user_ids.is_empty() {
            return Err(Status::invalid_argument("No valid user_ids provided"));
        }

        // æ‰¹é‡æŸ¥è¯¢ (ä½¿ç”¨ IN å­å¥)
        let users = sqlx::query_as!(
            UserModel,
            r#"
            SELECT id, username, email, display_name, avatar_url,
                   is_active, email_verified, created_at, updated_at
            FROM users
            WHERE id = ANY($1) AND deleted_at IS NULL
            "#,
            &user_ids
        )
        .fetch_all(&self.db_pool)
        .await
        .map_err(|e| Status::internal(format!("Database error: {}", e)))?;

        Ok(Response::new(GetUserBatchResponse {
            users: users.into_iter().map(|u| u.into()).collect(),
        }))
    }

    async fn check_user_exists(
        &self,
        request: Request<CheckUserExistsRequest>,
    ) -> Result<Response<CheckUserExistsResponse>, Status> {
        let user_id = Uuid::parse_str(&request.into_inner().user_id)
            .map_err(|_| Status::invalid_argument("Invalid user_id"))?;

        let exists = sqlx::query_scalar!(
            r#"SELECT EXISTS(SELECT 1 FROM users WHERE id = $1 AND deleted_at IS NULL)"#,
            user_id
        )
        .fetch_one(&self.db_pool)
        .await
        .map_err(|e| Status::internal(format!("Database error: {}", e)))?
        .unwrap_or(false);

        Ok(Response::new(CheckUserExistsResponse { exists }))
    }
}

impl AuthServiceImpl {
    async fn get_from_cache(&self, user_id: Uuid) -> Result<Option<User>, Status> {
        let key = format!("user:{}", user_id);
        let cached = self.cache.get(&key).await
            .map_err(|e| Status::internal(format!("Cache error: {}", e)))?;

        Ok(cached.and_then(|json| serde_json::from_str(&json).ok()))
    }

    async fn cache_user(&self, user: &UserModel) -> Result<(), Status> {
        let key = format!("user:{}", user.id);
        let json = serde_json::to_string(user)
            .map_err(|e| Status::internal(format!("Serialization error: {}", e)))?;

        self.cache.set_ex(&key, &json, 3600).await  // 1 hour TTL
            .map_err(|e| Status::internal(format!("Cache error: {}", e)))?;

        Ok(())
    }
}
```

**ä»»åŠ¡**:
- [ ] å®ç° 3 ä¸ª gRPC æ–¹æ³•
- [ ] æ·»åŠ  Redis ç¼“å­˜å±‚
- [ ] å•å…ƒæµ‹è¯•è¦†ç›–ç‡ > 80%
- [ ] é›†æˆæµ‹è¯• (gRPC å®¢æˆ·ç«¯è°ƒç”¨)
- [ ] æ€§èƒ½æµ‹è¯• (ç›®æ ‡: < 50ms p95)

---

#### [ ] 1.3 å®ç°äº‹ä»¶å‘å¸ƒ

**æ–‡ä»¶**: `backend/auth-service/src/events/user_events.rs`

```rust
use serde::{Deserialize, Serialize};
use uuid::Uuid;

#[derive(Debug, Serialize, Deserialize)]
#[serde(tag = "event_type")]
pub enum UserEvent {
    UserCreated(UserCreatedEvent),
    UserUpdated(UserUpdatedEvent),
    UserDeleted(UserDeletedEvent),
}

#[derive(Debug, Serialize, Deserialize)]
pub struct UserCreatedEvent {
    pub user_id: Uuid,
    pub username: String,
    pub email: String,
    pub display_name: Option<String>,
    pub avatar_url: Option<String>,
    pub created_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct UserUpdatedEvent {
    pub user_id: Uuid,
    pub username: Option<String>,
    pub email: Option<String>,
    pub display_name: Option<String>,
    pub avatar_url: Option<String>,
    pub updated_at: chrono::DateTime<chrono::Utc>,
}

#[derive(Debug, Serialize, Deserialize)]
pub struct UserDeletedEvent {
    pub user_id: Uuid,
    pub deleted_at: chrono::DateTime<chrono::Utc>,
}

pub async fn publish_user_event(
    event: UserEvent,
    kafka_producer: &FutureProducer,
) -> Result<(), Box<dyn std::error::Error>> {
    let topic = "nova.user.events";
    let key = match &event {
        UserEvent::UserCreated(e) => e.user_id.to_string(),
        UserEvent::UserUpdated(e) => e.user_id.to_string(),
        UserEvent::UserDeleted(e) => e.user_id.to_string(),
    };
    let payload = serde_json::to_string(&event)?;

    kafka_producer
        .send(
            FutureRecord::to(topic)
                .key(&key)
                .payload(&payload),
            Duration::from_secs(5),
        )
        .await
        .map_err(|(err, _)| err)?;

    tracing::info!(
        event_type = ?event,
        "Published user event to Kafka"
    );

    Ok(())
}
```

**ä¿®æ”¹ç”¨æˆ·åˆ›å»º/æ›´æ–°/åˆ é™¤å‡½æ•°**:
```rust
// ç¤ºä¾‹: ç”¨æˆ·åˆ›å»º
pub async fn create_user(
    db_pool: &PgPool,
    kafka_producer: &FutureProducer,
    input: CreateUserInput,
) -> Result<User, Error> {
    // 1. æ’å…¥æ•°æ®åº“
    let user = sqlx::query_as!(
        UserModel,
        r#"INSERT INTO users (...) VALUES (...) RETURNING *"#,
        // ...
    )
    .fetch_one(db_pool)
    .await?;

    // 2. å‘å¸ƒäº‹ä»¶
    publish_user_event(
        UserEvent::UserCreated(UserCreatedEvent {
            user_id: user.id,
            username: user.username.clone(),
            email: user.email.clone(),
            display_name: user.display_name.clone(),
            avatar_url: user.avatar_url.clone(),
            created_at: user.created_at,
        }),
        kafka_producer,
    )
    .await?;

    Ok(user.into())
}
```

**ä»»åŠ¡**:
- [ ] å®šä¹‰äº‹ä»¶ç»“æ„
- [ ] å®ç° Kafka å‘å¸ƒé€»è¾‘
- [ ] ä¿®æ”¹ CRUD å‡½æ•°ä»¥å‘å¸ƒäº‹ä»¶
- [ ] æµ‹è¯•äº‹ä»¶å‘å¸ƒ (æ¶ˆè´¹ç«¯éªŒè¯)

---

### [ ] Week 2: å…¶ä»–æœåŠ¡é›†æˆ

**è´Ÿè´£äºº**: Backend Engineer #2

#### [ ] 2.1 user-service é›†æˆ

**æ–‡ä»¶**: `backend/user-service/src/clients/auth_client.rs`

```rust
use proto::auth_service_client::AuthServiceClient;
use tonic::transport::Channel;

pub struct AuthClient {
    client: AuthServiceClient<Channel>,
}

impl AuthClient {
    pub async fn new(endpoint: &str) -> Result<Self, Box<dyn std::error::Error>> {
        let client = AuthServiceClient::connect(endpoint.to_string()).await?;
        Ok(Self { client })
    }

    pub async fn get_user(&mut self, user_id: Uuid) -> Result<User, Status> {
        let request = Request::new(GetUserRequest {
            user_id: user_id.to_string(),
        });

        let response = self.client.get_user(request).await?;
        response.into_inner().user.ok_or_else(|| Status::not_found("User not found"))
    }

    pub async fn check_user_exists(&mut self, user_id: Uuid) -> Result<bool, Status> {
        let request = Request::new(CheckUserExistsRequest {
            user_id: user_id.to_string(),
        });

        let response = self.client.check_user_exists(request).await?;
        Ok(response.into_inner().exists)
    }
}
```

**ä¿®æ”¹ç°æœ‰ä»£ç **:
```rust
// æ—§ä»£ç  (ç›´æ¥æŸ¥è¯¢ nova_staging.users)
let user = sqlx::query_as!(
    UserModel,
    "SELECT * FROM users WHERE id = $1",
    user_id
)
.fetch_one(&db_pool)
.await?;

// æ–°ä»£ç  (é€šè¿‡ gRPC è°ƒç”¨ auth-service)
let user = auth_client.get_user(user_id).await?;
```

**ä»»åŠ¡**:
- [ ] åˆ›å»º auth-service gRPC å®¢æˆ·ç«¯
- [ ] æ›¿æ¢æ‰€æœ‰ç›´æ¥æŸ¥è¯¢ `users` è¡¨çš„ä»£ç 
- [ ] æ·»åŠ é‡è¯•é€»è¾‘ (å¤±è´¥æ—¶é™çº§åˆ°ç¼“å­˜)
- [ ] æµ‹è¯•å®¢æˆ·ç«¯è°ƒç”¨

---

#### [ ] 2.2 è®¢é˜… Kafka äº‹ä»¶

**æ–‡ä»¶**: `backend/user-service/src/events/user_event_handler.rs`

```rust
use rdkafka::consumer::{Consumer, StreamConsumer};
use rdkafka::Message;

pub async fn start_user_event_consumer(
    kafka_consumer: Arc<StreamConsumer>,
    db_pool: PgPool,
) {
    kafka_consumer.subscribe(&["nova.user.events"]).unwrap();

    loop {
        match kafka_consumer.recv().await {
            Ok(message) => {
                if let Some(payload) = message.payload_view::<str>() {
                    match payload {
                        Ok(json) => {
                            if let Err(e) = handle_user_event(json, &db_pool).await {
                                tracing::error!(
                                    error = ?e,
                                    "Failed to handle user event"
                                );
                            }
                        }
                        Err(e) => {
                            tracing::error!(error = ?e, "Invalid UTF-8 payload");
                        }
                    }
                }
            }
            Err(e) => {
                tracing::error!(error = ?e, "Kafka consumer error");
            }
        }
    }
}

async fn handle_user_event(
    json: &str,
    db_pool: &PgPool,
) -> Result<(), Box<dyn std::error::Error>> {
    let event: UserEvent = serde_json::from_str(json)?;

    match event {
        UserEvent::UserCreated(e) => {
            // æ’å…¥ user_cache è¡¨
            sqlx::query!(
                r#"
                INSERT INTO user_cache (user_id, username, email, display_name, avatar_url, updated_at)
                VALUES ($1, $2, $3, $4, $5, $6)
                ON CONFLICT (user_id) DO UPDATE SET
                  username = EXCLUDED.username,
                  email = EXCLUDED.email,
                  display_name = EXCLUDED.display_name,
                  avatar_url = EXCLUDED.avatar_url,
                  updated_at = EXCLUDED.updated_at
                "#,
                e.user_id,
                e.username,
                e.email,
                e.display_name,
                e.avatar_url,
                e.created_at
            )
            .execute(db_pool)
            .await?;

            tracing::info!(user_id = %e.user_id, "Cached user from UserCreated event");
        }
        UserEvent::UserUpdated(e) => {
            // æ›´æ–° user_cache è¡¨
            sqlx::query!(
                r#"
                UPDATE user_cache
                SET username = COALESCE($2, username),
                    email = COALESCE($3, email),
                    display_name = COALESCE($4, display_name),
                    avatar_url = COALESCE($5, avatar_url),
                    updated_at = $6
                WHERE user_id = $1
                "#,
                e.user_id,
                e.username,
                e.email,
                e.display_name,
                e.avatar_url,
                e.updated_at
            )
            .execute(db_pool)
            .await?;

            tracing::info!(user_id = %e.user_id, "Updated user cache from UserUpdated event");
        }
        UserEvent::UserDeleted(e) => {
            // è½¯åˆ é™¤ user_cache
            sqlx::query!(
                r#"
                UPDATE user_cache
                SET deleted_at = $2
                WHERE user_id = $1
                "#,
                e.user_id,
                e.deleted_at
            )
            .execute(db_pool)
            .await?;

            tracing::info!(user_id = %e.user_id, "Soft-deleted user cache from UserDeleted event");
        }
    }

    Ok(())
}
```

**åˆ›å»º user_cache è¡¨**:
```sql
-- backend/user-service/migrations/0002_create_user_cache.sql
CREATE TABLE user_cache (
  user_id UUID PRIMARY KEY,
  username VARCHAR(255) NOT NULL,
  email VARCHAR(255) NOT NULL,
  display_name VARCHAR(255),
  avatar_url TEXT,
  updated_at TIMESTAMPTZ NOT NULL,
  deleted_at TIMESTAMPTZ,
  synced_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_user_cache_username ON user_cache(username) WHERE deleted_at IS NULL;
CREATE INDEX idx_user_cache_email ON user_cache(email) WHERE deleted_at IS NULL;
```

**ä»»åŠ¡**:
- [ ] åˆ›å»º user_cache è¡¨
- [ ] å®ç° Kafka æ¶ˆè´¹è€…
- [ ] æµ‹è¯•äº‹ä»¶å¤„ç†é€»è¾‘
- [ ] ç›‘æ§åŒæ­¥å»¶è¿Ÿ (ç›®æ ‡: < 1s p95)

---

#### [ ] 2.3 åˆ é™¤ nova_staging.users è¡¨

**âš ï¸ è­¦å‘Š**: è¿™æ˜¯ä¸å¯é€†æ“ä½œ,ç¡®ä¿æ‰€æœ‰æµ‹è¯•é€šè¿‡

**å‰ç½®æ¡ä»¶**:
- [ ] auth-service gRPC API å·²ä¸Šçº¿
- [ ] æ‰€æœ‰æœåŠ¡å·²è¿ç§»åˆ° gRPC è°ƒç”¨
- [ ] user_cache è¡¨æ•°æ®å·²åŒæ­¥
- [ ] ç”Ÿäº§ç¯å¢ƒæµ‹è¯•é€šè¿‡

**è¿ç§»è„šæœ¬**: `backend/migrations/0010_drop_staging_users.sql`

```sql
-- 1. å¤‡ä»½è¡¨æ•°æ®
CREATE TABLE users_backup AS SELECT * FROM users;

-- 2. éªŒè¯æ•°æ®ä¸€è‡´æ€§
DO $$
DECLARE
  auth_count INTEGER;
  staging_count INTEGER;
BEGIN
  SELECT COUNT(*) INTO auth_count FROM nova_auth.users;
  SELECT COUNT(*) INTO staging_count FROM nova_staging.users_backup;

  IF auth_count != staging_count THEN
    RAISE EXCEPTION 'User count mismatch: auth=%, staging=%', auth_count, staging_count;
  END IF;

  RAISE NOTICE 'Data validation passed: % users', auth_count;
END $$;

-- 3. åˆ é™¤å¤–é”®çº¦æŸ
ALTER TABLE user_profiles DROP CONSTRAINT fk_user_profiles_user;
ALTER TABLE user_settings DROP CONSTRAINT user_settings_user_id_fkey;
ALTER TABLE user_relationships DROP CONSTRAINT user_relationships_follower_id_fkey;
ALTER TABLE user_relationships DROP CONSTRAINT user_relationships_followee_id_fkey;
ALTER TABLE activity_logs DROP CONSTRAINT activity_logs_user_id_fkey;
ALTER TABLE reports DROP CONSTRAINT reports_reporter_id_fkey;
ALTER TABLE reports DROP CONSTRAINT reports_reported_user_id_fkey;
ALTER TABLE search_history DROP CONSTRAINT search_history_user_id_fkey;
ALTER TABLE search_suggestions DROP CONSTRAINT search_suggestions_user_id_fkey;
ALTER TABLE moderation_queue DROP CONSTRAINT moderation_queue_assigned_to_fkey;
ALTER TABLE moderation_actions DROP CONSTRAINT moderation_actions_moderator_id_fkey;
ALTER TABLE moderation_appeals DROP CONSTRAINT moderation_appeals_user_id_fkey;
ALTER TABLE moderation_appeals DROP CONSTRAINT moderation_appeals_reviewed_by_fkey;

-- 4. åˆ é™¤ users è¡¨
DROP TABLE users CASCADE;

-- 5. è®°å½•åˆ é™¤äº‹ä»¶
INSERT INTO migration_log (migration_name, executed_at, notes)
VALUES ('drop_staging_users', NOW(), 'Deleted nova_staging.users table after migrating to auth-service API');
```

**å›æ»šè„šæœ¬**: `backend/migrations/rollback_0010_drop_staging_users.sql`

```sql
-- æ¢å¤ users è¡¨
CREATE TABLE users AS SELECT * FROM users_backup;

-- æ¢å¤å¤–é”®çº¦æŸ
ALTER TABLE user_profiles
  ADD CONSTRAINT fk_user_profiles_user
  FOREIGN KEY (id) REFERENCES users(id) ON DELETE CASCADE;

-- (é‡å¤æ‰€æœ‰å¤–é”®çº¦æŸ)

-- åˆ é™¤å¤‡ä»½è¡¨
DROP TABLE users_backup;
```

**æ‰§è¡Œæ­¥éª¤**:
```bash
# 1. åœ¨æµ‹è¯•ç¯å¢ƒéªŒè¯
kubectl exec -n nova-staging postgres-... -- psql -U postgres -d nova_staging -f /migrations/0010_drop_staging_users.sql

# 2. è¿è¡Œé›†æˆæµ‹è¯•
cargo test --all

# 3. åœ¨ç”Ÿäº§ç¯å¢ƒæ‰§è¡Œ (ç»´æŠ¤çª—å£)
kubectl exec -n nova postgres-... -- psql -U postgres -d nova_staging -f /migrations/0010_drop_staging_users.sql

# 4. ç›‘æ§é”™è¯¯æ—¥å¿—
kubectl logs -n nova -l app=user-service --tail=100 -f
```

**ä»»åŠ¡**:
- [ ] ç¼–å†™è¿ç§»è„šæœ¬
- [ ] ç¼–å†™å›æ»šè„šæœ¬
- [ ] æµ‹è¯•ç¯å¢ƒéªŒè¯
- [ ] ç”Ÿäº§ç¯å¢ƒæ‰§è¡Œ

---

## ğŸ“‹ Week 3-6: æ•°æ®åº“æ‹†åˆ†

### [ ] Week 3: åˆ›å»ºæ–°æ•°æ®åº“

**è´Ÿè´£äºº**: DevOps Engineer

#### [ ] 3.1 åˆ›å»ºæ•°æ®åº“å®ä¾‹

**åŸºç¡€è®¾æ–½ä»£ç **: `infrastructure/terraform/databases.tf`

```hcl
# nova_user æ•°æ®åº“
resource "aws_db_instance" "nova_user" {
  identifier           = "nova-user-db"
  engine               = "postgres"
  engine_version       = "16.3"
  instance_class       = "db.t3.medium"
  allocated_storage    = 100
  storage_type         = "gp3"
  storage_encrypted    = true

  db_name  = "nova_user"
  username = var.db_username
  password = var.db_password

  vpc_security_group_ids = [aws_security_group.nova_db.id]
  db_subnet_group_name   = aws_db_subnet_group.nova.name

  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "Mon:04:00-Mon:05:00"

  skip_final_snapshot       = false
  final_snapshot_identifier = "nova-user-db-final-snapshot"

  tags = {
    Name        = "nova-user-database"
    Environment = "production"
    Service     = "user-service"
  }
}

# nova_moderation æ•°æ®åº“
resource "aws_db_instance" "nova_moderation" {
  identifier           = "nova-moderation-db"
  engine               = "postgres"
  engine_version       = "16.3"
  instance_class       = "db.t3.small"
  allocated_storage    = 50
  storage_type         = "gp3"
  storage_encrypted    = true

  db_name  = "nova_moderation"
  username = var.db_username
  password = var.db_password

  vpc_security_group_ids = [aws_security_group.nova_db.id]
  db_subnet_group_name   = aws_db_subnet_group.nova.name

  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "Mon:04:00-Mon:05:00"

  skip_final_snapshot       = false
  final_snapshot_identifier = "nova-moderation-db-final-snapshot"

  tags = {
    Name        = "nova-moderation-database"
    Environment = "production"
    Service     = "moderation-service"
  }
}

# é‡å¤å…¶ä»–æ•°æ®åº“ (nova_search, nova_audit, nova_events)
```

**ä»»åŠ¡**:
- [ ] å®šä¹‰ Terraform èµ„æº
- [ ] åˆ›å»ºæ•°æ®åº“å®ä¾‹ (æµ‹è¯•ç¯å¢ƒ)
- [ ] éªŒè¯è¿æ¥æ€§
- [ ] åˆ›å»ºç”Ÿäº§ç¯å¢ƒå®ä¾‹

---

#### [ ] 3.2 è¿ç§»è¡¨ç»“æ„

**è¿ç§»è„šæœ¬**: `backend/scripts/migrate_tables.sh`

```bash
#!/bin/bash
set -e

SOURCE_DB="nova_staging"
TARGET_DB="nova_user"
TABLES=("user_profiles" "user_settings" "user_relationships")

for table in "${TABLES[@]}"; do
  echo "=== Migrating $table ==="

  # 1. å¯¼å‡ºè¡¨ç»“æ„
  pg_dump -U postgres -h $SOURCE_HOST -d $SOURCE_DB \
    -t $table --schema-only > /tmp/${table}_schema.sql

  # 2. å¯¼å…¥åˆ°æ–°æ•°æ®åº“
  psql -U postgres -h $TARGET_HOST -d $TARGET_DB \
    -f /tmp/${table}_schema.sql

  # 3. å¤åˆ¶æ•°æ®
  pg_dump -U postgres -h $SOURCE_HOST -d $SOURCE_DB \
    -t $table --data-only > /tmp/${table}_data.sql

  psql -U postgres -h $TARGET_HOST -d $TARGET_DB \
    -f /tmp/${table}_data.sql

  # 4. éªŒè¯è¡Œæ•°
  SOURCE_COUNT=$(psql -U postgres -h $SOURCE_HOST -d $SOURCE_DB \
    -t -c "SELECT COUNT(*) FROM $table")

  TARGET_COUNT=$(psql -U postgres -h $TARGET_HOST -d $TARGET_DB \
    -t -c "SELECT COUNT(*) FROM $table")

  if [ "$SOURCE_COUNT" != "$TARGET_COUNT" ]; then
    echo "âŒ Row count mismatch for $table: source=$SOURCE_COUNT, target=$TARGET_COUNT"
    exit 1
  fi

  echo "âœ… Migrated $table: $SOURCE_COUNT rows"
done

echo "=== Migration Complete ==="
```

**ä»»åŠ¡**:
- [ ] è¿ç§» user-service è¡¨
- [ ] è¿ç§» moderation-service è¡¨
- [ ] è¿ç§» search-service è¡¨
- [ ] éªŒè¯æ•°æ®å®Œæ•´æ€§

---

### [ ] Week 4-5: åŒå†™å®ç°

**è´Ÿè´£äºº**: Backend Engineer #1

#### [ ] 4.1 å®ç°åŒå†™é€»è¾‘

**æ–‡ä»¶**: `backend/user-service/src/repository/dual_write.rs`

```rust
pub struct DualWriteRepository {
    old_pool: PgPool,  // nova_staging
    new_pool: PgPool,  // nova_user
    feature_flag: Arc<FeatureFlags>,
}

impl DualWriteRepository {
    pub async fn insert_user_profile(
        &self,
        profile: &UserProfile,
    ) -> Result<(), Error> {
        // 1. å†™å…¥æ—§æ•°æ®åº“ (å¿…é¡»æˆåŠŸ)
        sqlx::query!(
            "INSERT INTO user_profiles (...) VALUES (...)",
            // ...
        )
        .execute(&self.old_pool)
        .await?;

        // 2. å†™å…¥æ–°æ•°æ®åº“ (å¯å¤±è´¥,è®°å½•é”™è¯¯)
        if let Err(e) = sqlx::query!(
            "INSERT INTO user_profiles (...) VALUES (...)",
            // ...
        )
        .execute(&self.new_pool)
        .await
        {
            tracing::error!(
                error = ?e,
                profile_id = %profile.id,
                "Failed to write to new database"
            );

            // è®°å½•åˆ°å¯¹è´¦è¡¨
            self.record_sync_failure(profile.id, "insert").await?;
        }

        Ok(())
    }

    pub async fn read_user_profile(
        &self,
        profile_id: Uuid,
    ) -> Result<UserProfile, Error> {
        // æ ¹æ®ç‰¹æ€§å¼€å…³å†³å®šè¯»å–å“ªä¸ªæ•°æ®åº“
        if self.feature_flag.is_enabled("use_new_user_db") {
            self.read_from_new_db(profile_id).await
        } else {
            self.read_from_old_db(profile_id).await
        }
    }
}
```

**å¯¹è´¦è¡¨**:
```sql
CREATE TABLE dual_write_sync_failures (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  entity_type VARCHAR(50) NOT NULL,
  entity_id UUID NOT NULL,
  operation VARCHAR(20) NOT NULL,  -- insert, update, delete
  error_message TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  resolved_at TIMESTAMPTZ
);

CREATE INDEX idx_sync_failures_unresolved
  ON dual_write_sync_failures(entity_type, created_at)
  WHERE resolved_at IS NULL;
```

**ä»»åŠ¡**:
- [ ] å®ç°åŒå†™ Repository
- [ ] æ·»åŠ ç‰¹æ€§å¼€å…³
- [ ] åˆ›å»ºå¯¹è´¦è¡¨
- [ ] æµ‹è¯•åŒå†™é€»è¾‘

---

#### [ ] 4.2 æ•°æ®å¯¹è´¦è„šæœ¬

**æ–‡ä»¶**: `backend/scripts/reconcile_dual_write.sh`

```bash
#!/bin/bash
set -e

echo "=== Dual Write Data Reconciliation ==="

OLD_DB="nova_staging"
NEW_DB="nova_user"

# æ£€æŸ¥æœªè§£å†³çš„åŒæ­¥å¤±è´¥
FAILURES=$(psql -U postgres -d $NEW_DB -t -c "
  SELECT COUNT(*)
  FROM dual_write_sync_failures
  WHERE resolved_at IS NULL
")

echo "Unresolved sync failures: $FAILURES"

if [ "$FAILURES" -gt 0 ]; then
  # å¯¹è´¦å¹¶ä¿®å¤
  psql -U postgres -d $NEW_DB -c "
    SELECT entity_type, entity_id, operation, error_message
    FROM dual_write_sync_failures
    WHERE resolved_at IS NULL
    ORDER BY created_at DESC
    LIMIT 100
  "

  # æ‰‹åŠ¨ä¿®å¤æˆ–è‡ªåŠ¨é‡è¯•
  # ...
fi

# éªŒè¯æ•°æ®ä¸€è‡´æ€§
for table in user_profiles user_settings user_relationships; do
  OLD_COUNT=$(psql -U postgres -h $OLD_HOST -d $OLD_DB -t -c "SELECT COUNT(*) FROM $table")
  NEW_COUNT=$(psql -U postgres -h $NEW_HOST -d $NEW_DB -t -c "SELECT COUNT(*) FROM $table")

  if [ "$OLD_COUNT" != "$NEW_COUNT" ]; then
    echo "âŒ $table: old=$OLD_COUNT, new=$NEW_COUNT (mismatch!)"
  else
    echo "âœ… $table: $OLD_COUNT rows"
  fi
done

echo "=== Reconciliation Complete ==="
```

**Cron å®šæ—¶ä»»åŠ¡**:
```yaml
# k8s/cronjobs/dual-write-reconcile.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: dual-write-reconcile
  namespace: nova
spec:
  schedule: "*/5 * * * *"  # æ¯ 5 åˆ†é’Ÿ
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: reconcile
            image: postgres:16
            command: ["/scripts/reconcile_dual_write.sh"]
            volumeMounts:
            - name: scripts
              mountPath: /scripts
          restartPolicy: OnFailure
          volumes:
          - name: scripts
            configMap:
              name: reconcile-scripts
```

**ä»»åŠ¡**:
- [ ] ç¼–å†™å¯¹è´¦è„šæœ¬
- [ ] åˆ›å»º CronJob
- [ ] æµ‹è¯•å¯¹è´¦é€»è¾‘
- [ ] ç›‘æ§å¯¹è´¦ç»“æœ

---

### [ ] Week 6: æµé‡åˆ‡æ¢

**è´Ÿè´£äºº**: DevOps Engineer

#### [ ] 6.1 é€æ­¥å¢åŠ æ–°æ•°æ®åº“æµé‡

**ç‰¹æ€§å¼€å…³é…ç½®**:
```yaml
# backend/user-service/config/feature_flags.yaml
feature_flags:
  use_new_user_db:
    enabled: true
    rollout_percentage: 10  # å¼€å§‹æ—¶ 10%
    whitelist_user_ids:     # å†…éƒ¨æµ‹è¯•ç”¨æˆ·
      - "uuid-1"
      - "uuid-2"
```

**ç›‘æ§æŒ‡æ ‡**:
```promql
# æŸ¥è¯¢å»¶è¿Ÿå¯¹æ¯”
histogram_quantile(0.95,
  rate(user_service_query_duration_seconds_bucket{db="old"}[5m])
) vs
histogram_quantile(0.95,
  rate(user_service_query_duration_seconds_bucket{db="new"}[5m])
)

# é”™è¯¯ç‡å¯¹æ¯”
rate(user_service_query_errors_total{db="old"}[5m]) vs
rate(user_service_query_errors_total{db="new"}[5m])
```

**åˆ‡æ¢è®¡åˆ’**:
```
Day 1: 10% æµé‡ â†’ è§‚å¯Ÿ 24 å°æ—¶
Day 3: 25% æµé‡ â†’ è§‚å¯Ÿ 24 å°æ—¶
Day 5: 50% æµé‡ â†’ è§‚å¯Ÿ 48 å°æ—¶
Day 8: 75% æµé‡ â†’ è§‚å¯Ÿ 24 å°æ—¶
Day 10: 100% æµé‡ â†’ è§‚å¯Ÿ 1 å‘¨
```

**ä»»åŠ¡**:
- [ ] é…ç½®ç‰¹æ€§å¼€å…³
- [ ] è®¾ç½®ç›‘æ§ä»ªè¡¨æ¿
- [ ] é€æ­¥å¢åŠ æµé‡ç™¾åˆ†æ¯”
- [ ] éªŒè¯æ€§èƒ½æŒ‡æ ‡

---

#### [ ] 6.2 åœæ­¢åŒå†™å¹¶æ¸…ç†

**âš ï¸ è­¦å‘Š**: ç¡®ä¿ 100% æµé‡å·²åˆ‡æ¢åˆ°æ–°æ•°æ®åº“

**å‰ç½®æ¡ä»¶**:
- [ ] æ–°æ•°æ®åº“æµé‡ = 100% (æŒç»­ 1 å‘¨)
- [ ] æ— å¯¹è´¦å¤±è´¥è®°å½•
- [ ] æ€§èƒ½æŒ‡æ ‡æ­£å¸¸

**æ¸…ç†æ­¥éª¤**:
```bash
# 1. åœæ­¢åŒå†™ (ä¿®æ”¹ä»£ç )
# ç§»é™¤ DualWriteRepository,ä½¿ç”¨ SingleWriteRepository

# 2. åˆ é™¤æ—§è¡¨ (ä¿ç•™å¤‡ä»½ 2 å‘¨)
psql -U postgres -d nova_staging -c "
  CREATE TABLE user_profiles_backup AS SELECT * FROM user_profiles;
  DROP TABLE user_profiles;
"

# 3. æ¸…ç†å¯¹è´¦è¡¨
psql -U postgres -d nova_user -c "
  DELETE FROM dual_write_sync_failures
  WHERE resolved_at IS NOT NULL
    AND resolved_at < NOW() - INTERVAL '30 days';
"
```

**ä»»åŠ¡**:
- [ ] ç§»é™¤åŒå†™é€»è¾‘
- [ ] åˆ é™¤æ—§è¡¨ (ä¿ç•™å¤‡ä»½)
- [ ] æ¸…ç†å¯¹è´¦æ•°æ®
- [ ] æ›´æ–°æ–‡æ¡£

---

## ğŸ“‹ Week 7-8: æ¶ˆé™¤å¤–é”® + Saga

### [ ] Week 7: åˆ é™¤è·¨æœåŠ¡å¤–é”®

**è´Ÿè´£äºº**: Backend Engineer #2

#### [ ] 7.1 è¯†åˆ«å¹¶åˆ é™¤å¤–é”®çº¦æŸ

**è„šæœ¬**: `backend/scripts/drop_cross_service_fks.sql`

```sql
-- 1. å¤‡ä»½å¤–é”®ä¿¡æ¯
CREATE TABLE fk_backup AS
SELECT
  tc.table_name,
  kcu.column_name,
  tc.constraint_name,
  ccu.table_name AS foreign_table_name,
  ccu.column_name AS foreign_column_name,
  rc.delete_rule
FROM information_schema.table_constraints AS tc
JOIN information_schema.key_column_usage AS kcu
  ON tc.constraint_name = kcu.constraint_name
JOIN information_schema.constraint_column_usage AS ccu
  ON ccu.constraint_name = tc.constraint_name
JOIN information_schema.referential_constraints AS rc
  ON rc.constraint_name = tc.constraint_name
WHERE tc.constraint_type = 'FOREIGN KEY';

-- 2. åˆ é™¤è·¨æœåŠ¡å¤–é”® (nova_moderation.reports)
ALTER TABLE reports DROP CONSTRAINT reports_reporter_id_fkey;
ALTER TABLE reports DROP CONSTRAINT reports_reported_user_id_fkey;

-- 3. æ·»åŠ ç´¢å¼•ä»¥ä¿æŒæŸ¥è¯¢æ€§èƒ½
CREATE INDEX idx_reports_reporter_id ON reports(reporter_id);
CREATE INDEX idx_reports_reported_user_id ON reports(reported_user_id);

-- 4. é‡å¤å…¶ä»–è·¨æœåŠ¡å¤–é”®
-- activity_logs, search_history, moderation_queue, etc.
```

**ä»»åŠ¡**:
- [ ] å¤‡ä»½å¤–é”®ä¿¡æ¯
- [ ] åˆ é™¤æ‰€æœ‰è·¨æœåŠ¡å¤–é”®
- [ ] æ·»åŠ ç´¢å¼•
- [ ] éªŒè¯æŸ¥è¯¢æ€§èƒ½

---

#### [ ] 7.2 åº”ç”¨å±‚éªŒè¯

**æ–‡ä»¶**: `backend/moderation-service/src/validators/user_validator.rs`

```rust
use proto::auth_service_client::AuthServiceClient;

pub struct UserValidator {
    auth_client: AuthServiceClient<Channel>,
}

impl UserValidator {
    pub async fn validate_user_exists(
        &mut self,
        user_id: Uuid,
    ) -> Result<(), Error> {
        let exists = self.auth_client
            .check_user_exists(CheckUserExistsRequest {
                user_id: user_id.to_string(),
            })
            .await
            .map_err(|e| Error::AuthServiceUnavailable(e.to_string()))?
            .into_inner()
            .exists;

        if !exists {
            return Err(Error::UserNotFound(user_id));
        }

        Ok(())
    }
}

// ä½¿ç”¨ç¤ºä¾‹
pub async fn create_report(
    validator: &mut UserValidator,
    input: CreateReportInput,
) -> Result<Report, Error> {
    // 1. éªŒè¯ç”¨æˆ·å­˜åœ¨ (æ›¿ä»£å¤–é”®çº¦æŸ)
    validator.validate_user_exists(input.reporter_id).await?;
    validator.validate_user_exists(input.reported_user_id).await?;

    // 2. æ’å…¥ report
    let report = sqlx::query_as!(
        ReportModel,
        r#"
        INSERT INTO reports (reporter_id, reported_user_id, ...)
        VALUES ($1, $2, ...)
        RETURNING *
        "#,
        input.reporter_id,
        input.reported_user_id,
        // ...
    )
    .fetch_one(&db_pool)
    .await?;

    Ok(report.into())
}
```

**ä»»åŠ¡**:
- [ ] å®ç° UserValidator
- [ ] æ›¿æ¢æ‰€æœ‰ä¾èµ–å¤–é”®çš„ä»£ç 
- [ ] æ·»åŠ é™çº§é€»è¾‘ (auth-service ä¸å¯ç”¨æ—¶)
- [ ] æµ‹è¯•éªŒè¯é€»è¾‘

---

### [ ] Week 8: Saga æ¨¡å¼å®ç°

**è´Ÿè´£äºº**: Backend Engineer #1 + #2

#### [ ] 8.1 Saga æ¡†æ¶

**æ–‡ä»¶**: `backend/libs/saga/src/lib.rs`

```rust
use async_trait::async_trait;
use std::collections::HashMap;
use std::sync::Arc;

#[async_trait]
pub trait SagaStep: Send + Sync {
    async fn execute(&self) -> Result<(), Box<dyn std::error::Error>>;
    async fn compensate(&self) -> Result<(), Box<dyn std::error::Error>>;
}

pub struct Saga {
    name: String,
    aggregate_id: Uuid,
    steps: Vec<Box<dyn SagaStep>>,
    state: SagaState,
}

#[derive(Debug, Clone)]
pub enum SagaState {
    Pending,
    Running,
    Completed,
    Compensating,
    Failed,
}

impl Saga {
    pub fn new(name: &str, aggregate_id: Uuid) -> Self {
        Self {
            name: name.to_string(),
            aggregate_id,
            steps: Vec::new(),
            state: SagaState::Pending,
        }
    }

    pub fn add_step(&mut self, step: Box<dyn SagaStep>) {
        self.steps.push(step);
    }

    pub async fn execute(&mut self) -> Result<(), SagaError> {
        self.state = SagaState::Running;
        let mut completed_steps = 0;

        for (i, step) in self.steps.iter().enumerate() {
            match step.execute().await {
                Ok(_) => {
                    completed_steps += 1;
                    tracing::info!(
                        saga = %self.name,
                        step = i,
                        "Saga step completed"
                    );
                }
                Err(e) => {
                    tracing::error!(
                        saga = %self.name,
                        step = i,
                        error = ?e,
                        "Saga step failed, starting compensation"
                    );

                    self.state = SagaState::Compensating;
                    self.compensate(completed_steps).await?;

                    self.state = SagaState::Failed;
                    return Err(SagaError::StepFailed {
                        step: i,
                        error: e.to_string(),
                    });
                }
            }
        }

        self.state = SagaState::Completed;
        Ok(())
    }

    async fn compensate(&self, steps_to_compensate: usize) -> Result<(), SagaError> {
        for i in (0..steps_to_compensate).rev() {
            if let Err(e) = self.steps[i].compensate().await {
                tracing::error!(
                    saga = %self.name,
                    step = i,
                    error = ?e,
                    "Compensation failed"
                );

                return Err(SagaError::CompensationFailed {
                    step: i,
                    error: e.to_string(),
                });
            }
        }

        Ok(())
    }
}

#[derive(Debug, thiserror::Error)]
pub enum SagaError {
    #[error("Saga step {step} failed: {error}")]
    StepFailed { step: usize, error: String },

    #[error("Compensation for step {step} failed: {error}")]
    CompensationFailed { step: usize, error: String },
}
```

**ä»»åŠ¡**:
- [ ] å®ç° Saga æ¡†æ¶
- [ ] æ·»åŠ çŠ¶æ€æŒä¹…åŒ– (saga_state è¡¨)
- [ ] æ·»åŠ é‡è¯•é€»è¾‘
- [ ] å•å…ƒæµ‹è¯•

---

#### [ ] 8.2 ç”¨æˆ·åˆ é™¤ Saga

**æ–‡ä»¶**: `backend/user-service/src/sagas/delete_user_saga.rs`

```rust
use saga::{Saga, SagaStep};

struct SoftDeleteUserProfileStep {
    user_id: Uuid,
    db_pool: PgPool,
}

#[async_trait]
impl SagaStep for SoftDeleteUserProfileStep {
    async fn execute(&self) -> Result<(), Box<dyn std::error::Error>> {
        sqlx::query!(
            "UPDATE user_profiles SET deleted_at = NOW() WHERE id = $1",
            self.user_id
        )
        .execute(&self.db_pool)
        .await?;

        Ok(())
    }

    async fn compensate(&self) -> Result<(), Box<dyn std::error::Error>> {
        sqlx::query!(
            "UPDATE user_profiles SET deleted_at = NULL WHERE id = $1",
            self.user_id
        )
        .execute(&self.db_pool)
        .await?;

        Ok(())
    }
}

struct ArchiveUserReportsStep {
    user_id: Uuid,
    moderation_client: ModerationServiceClient<Channel>,
}

#[async_trait]
impl SagaStep for ArchiveUserReportsStep {
    async fn execute(&self) -> Result<(), Box<dyn std::error::Error>> {
        self.moderation_client
            .archive_user_reports(ArchiveUserReportsRequest {
                user_id: self.user_id.to_string(),
            })
            .await?;

        Ok(())
    }

    async fn compensate(&self) -> Result<(), Box<dyn std::error::Error>> {
        self.moderation_client
            .restore_user_reports(RestoreUserReportsRequest {
                user_id: self.user_id.to_string(),
            })
            .await?;

        Ok(())
    }
}

pub async fn delete_user_saga(
    user_id: Uuid,
    db_pool: PgPool,
    moderation_client: ModerationServiceClient<Channel>,
    search_client: SearchServiceClient<Channel>,
    auth_client: AuthServiceClient<Channel>,
) -> Result<(), SagaError> {
    let mut saga = Saga::new("delete_user", user_id);

    // Step 1: è½¯åˆ é™¤ç”¨æˆ·èµ„æ–™
    saga.add_step(Box::new(SoftDeleteUserProfileStep {
        user_id,
        db_pool: db_pool.clone(),
    }));

    // Step 2: å½’æ¡£å®¡æ ¸æ•°æ®
    saga.add_step(Box::new(ArchiveUserReportsStep {
        user_id,
        moderation_client,
    }));

    // Step 3: åˆ é™¤æœç´¢å†å²
    saga.add_step(Box::new(DeleteUserSearchHistoryStep {
        user_id,
        search_client,
    }));

    // Step 4: åˆ é™¤è®¤è¯è´¦æˆ· (æœ€åä¸€æ­¥,ä¸å¯å›æ»š)
    saga.add_step(Box::new(DeleteAuthAccountStep {
        user_id,
        auth_client,
    }));

    saga.execute().await
}
```

**ä»»åŠ¡**:
- [ ] å®ç°ç”¨æˆ·åˆ é™¤ Saga
- [ ] å®ç°æ‰€æœ‰ Saga Steps
- [ ] æµ‹è¯•æ­£å¸¸æµç¨‹
- [ ] æµ‹è¯•è¡¥å¿æµç¨‹

---

#### [ ] 8.3 Saga çŠ¶æ€æŒä¹…åŒ–

**è¡¨ç»“æ„**:
```sql
CREATE TABLE saga_state (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  saga_name VARCHAR(100) NOT NULL,
  aggregate_id UUID NOT NULL,
  state VARCHAR(50) NOT NULL,
  current_step INTEGER NOT NULL DEFAULT 0,
  total_steps INTEGER NOT NULL,
  error_message TEXT,
  created_at TIMESTAMPTZ DEFAULT NOW(),
  updated_at TIMESTAMPTZ DEFAULT NOW(),
  completed_at TIMESTAMPTZ
);

CREATE INDEX idx_saga_state_pending
  ON saga_state(state, created_at)
  WHERE state = 'Running';

CREATE INDEX idx_saga_state_failed
  ON saga_state(state, created_at)
  WHERE state = 'Failed';
```

**åå°é‡è¯•ä»»åŠ¡**:
```rust
pub async fn retry_failed_sagas(db_pool: PgPool) {
    loop {
        let failed_sagas = sqlx::query_as!(
            SagaStateModel,
            r#"
            SELECT *
            FROM saga_state
            WHERE state = 'Failed'
              AND created_at > NOW() - INTERVAL '24 hours'
            ORDER BY created_at
            LIMIT 10
            "#
        )
        .fetch_all(&db_pool)
        .await
        .unwrap_or_default();

        for saga_state in failed_sagas {
            tracing::info!(
                saga_id = %saga_state.id,
                saga_name = %saga_state.saga_name,
                "Retrying failed saga"
            );

            // é‡å»º Saga å¹¶é‡è¯•
            // ...
        }

        tokio::time::sleep(Duration::from_secs(60)).await;
    }
}
```

**ä»»åŠ¡**:
- [ ] åˆ›å»º saga_state è¡¨
- [ ] å®ç°çŠ¶æ€æŒä¹…åŒ–
- [ ] å®ç°åå°é‡è¯•ä»»åŠ¡
- [ ] ç›‘æ§ Saga æˆåŠŸç‡

---

## âœ… éªŒæ”¶æ ‡å‡†

### æŠ€æœ¯æŒ‡æ ‡

- [ ] æ¯ä¸ªæœåŠ¡ç‹¬ç«‹æ‹¥æœ‰æ•°æ®åº“
- [ ] é›¶è·¨æœåŠ¡å¤–é”®çº¦æŸ
- [ ] äº‹ä»¶åŒæ­¥å»¶è¿Ÿ < 1s (p95)
- [ ] æŸ¥è¯¢æ€§èƒ½ < 100ms (p95)
- [ ] æ•°æ®ä¸€è‡´æ€§ > 99.99%
- [ ] Saga æˆåŠŸç‡ > 99%

### ä¸šåŠ¡æŒ‡æ ‡

- [ ] é›¶æ•°æ®ä¸¢å¤±
- [ ] é›¶åœæœºè¿ç§»
- [ ] ç”¨æˆ·ä½“éªŒæ— é™çº§
- [ ] æˆæœ¬å¢åŠ  < $1000/æœˆ

### æ–‡æ¡£å®Œæ•´æ€§

- [ ] æŠ€æœ¯è®¾è®¡æ–‡æ¡£ (TDD)
- [ ] API è§„èŒƒ (Protobuf)
- [ ] è¿ç§» Runbook
- [ ] å›æ»š Playbook
- [ ] ç›‘æ§ä»ªè¡¨æ¿
- [ ] å‘Šè­¦è§„åˆ™

---

## ğŸ“ è”ç³»æ–¹å¼

### é¡¹ç›®å›¢é˜Ÿ

- **é¡¹ç›®ç»ç†**: [å§“å] (Slack: @pm)
- **Backend Lead**: [å§“å] (Slack: @backend-lead)
- **DevOps Lead**: [å§“å] (Slack: @devops-lead)

### å…³é”®ä¼šè®®

- **æ¯æ—¥ç«™ä¼š**: 10:00 AM (15 åˆ†é’Ÿ)
- **å‘¨ä¸­å®¡æŸ¥**: æ¯å‘¨ä¸‰ 14:00 (1 å°æ—¶)
- **å‘¨æœ«å›é¡¾**: æ¯å‘¨äº” 16:00 (1 å°æ—¶)

---

**æœ€åæ›´æ–°**: 2025-11-11
**ä¸‹æ¬¡å®¡æŸ¥**: Week 2 Checkpoint (2025-11-25)

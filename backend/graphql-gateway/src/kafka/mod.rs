//! Kafka Event Streaming Integration
//! âœ… P0-5: Production-ready subscription event handling
//!
//! This module provides:
//! - Kafka consumer for subscription events (feed, messages, notifications)
//! - Filtering and routing of events to subscriptions
//! - Connection pooling and error handling
//! - Event serialization/deserialization
//!
//! Topics:
//! - feed.events: Feed updates (posts, likes, etc)
//! - messaging.events: Direct messages
//! - notification.events: Notifications (likes, follows, mentions)

// Kafka infrastructure prepared but not yet integrated with WebSocket subscriptions
#![allow(dead_code)]

use rdkafka::consumer::{Consumer, StreamConsumer};
use rdkafka::ClientConfig;
use std::sync::Arc;
use thiserror::Error;
use tokio::sync::RwLock;
use tracing::{error, info};

pub mod consumer;
pub mod producer;

pub use producer::KafkaProducer;

/// Kafka configuration
#[derive(Debug, Clone)]
pub struct KafkaConfig {
    pub brokers: Vec<String>,
    pub group_id: String,
    pub timeout_ms: u64,
    pub auto_offset_reset: String,
}

impl Default for KafkaConfig {
    fn default() -> Self {
        Self {
            brokers: vec!["localhost:9092".to_string()],
            group_id: "graphql-gateway".to_string(),
            timeout_ms: 5000,
            auto_offset_reset: "earliest".to_string(),
        }
    }
}

/// Kafka integration error types
#[derive(Debug, Error)]
pub enum KafkaError {
    #[error("Connection failed: {0}")]
    ConnectionFailed(String),

    #[error("Consumer error: {0}")]
    ConsumerError(String),

    #[error("Producer error: {0}")]
    ProducerError(String),

    #[error("Deserialization error: {0}")]
    DeserializationError(String),

    #[error("Serialization error: {0}")]
    SerializationError(String),

    #[error("Timeout")]
    Timeout,
}

/// Kafka subscription manager
/// Coordinates Kafka consumers with GraphQL subscriptions
pub struct KafkaSubscriptionManager {
    config: KafkaConfig,
    consumer: Arc<RwLock<Option<StreamConsumer>>>,
    producer: Arc<RwLock<Option<KafkaProducer>>>,
}

impl KafkaSubscriptionManager {
    /// Create new Kafka subscription manager
    pub fn new(config: KafkaConfig) -> Self {
        Self {
            config,
            consumer: Arc::new(RwLock::new(None)),
            producer: Arc::new(RwLock::new(None)),
        }
    }

    /// Initialize Kafka connection
    pub async fn initialize(&self) -> Result<(), KafkaError> {
        info!(
            brokers = ?self.config.brokers,
            "Initializing Kafka connection"
        );

        // Create consumer
        let brokers_str = self.config.brokers.join(",");

        let consumer: StreamConsumer = ClientConfig::new()
            .set("bootstrap.servers", &brokers_str)
            .set("group.id", &self.config.group_id)
            .set("auto.offset.reset", &self.config.auto_offset_reset)
            .set("session.timeout.ms", "6000")
            .set("enable.auto.commit", "true")
            .create()
            .map_err(|e| KafkaError::ConnectionFailed(e.to_string()))?;

        let mut consumer_lock = self.consumer.write().await;
        *consumer_lock = Some(consumer);

        info!("Kafka connection established");
        Ok(())
    }

    /// Check connection status
    pub async fn is_healthy(&self) -> bool {
        match self.consumer.read().await.as_ref() {
            Some(consumer) => {
                // Simple check: try to fetch metadata
                consumer
                    .fetch_metadata(None, std::time::Duration::from_secs(2))
                    .is_ok()
            }
            None => false,
        }
    }

    /// Close Kafka connection
    pub async fn shutdown(&self) -> Result<(), KafkaError> {
        info!("Shutting down Kafka connection");

        let mut consumer_lock = self.consumer.write().await;
        *consumer_lock = None;

        let mut producer_lock = self.producer.write().await;
        *producer_lock = None;

        Ok(())
    }

    /// Check if consumer exists
    pub async fn has_consumer(&self) -> bool {
        self.consumer.read().await.is_some()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_kafka_config_default() {
        let config = KafkaConfig::default();
        assert_eq!(config.brokers, vec!["localhost:9092"]);
        assert_eq!(config.group_id, "graphql-gateway");
    }

    #[test]
    fn test_kafka_config_custom() {
        let config = KafkaConfig {
            brokers: vec!["kafka-1:9092".to_string(), "kafka-2:9092".to_string()],
            group_id: "custom-group".to_string(),
            timeout_ms: 10000,
            auto_offset_reset: "latest".to_string(),
        };

        assert_eq!(config.brokers.len(), 2);
        assert_eq!(config.group_id, "custom-group");
        assert_eq!(config.timeout_ms, 10000);
    }

    #[test]
    fn test_kafka_error_display() {
        let err = KafkaError::ConnectionFailed("test".to_string());
        assert!(err.to_string().contains("Connection failed"));
    }
}

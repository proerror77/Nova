# ClickHouse Analytics Infrastructure

**Version**: 1.0.0
**Status**: Production Ready
**Purpose**: Real-time event analytics and feed ranking for Nova Social Platform

---

## ğŸ“‹ Overview

è¿™æ˜¯ Nova å¹³å°çš„ ClickHouse OLAP æ•°æ®ä»“åº“é…ç½®ï¼Œç”¨äºï¼š

- **å®æ—¶äº‹ä»¶è¿½è¸ª**ï¼šç”¨æˆ·è¡Œä¸ºæ•°æ®ï¼ˆæµè§ˆã€ç‚¹èµã€è¯„è®ºã€åˆ†äº«ï¼‰
- **Feed æ’åº**ï¼šåŸºäºç”¨æˆ·åå¥½å’Œå†…å®¹è´¨é‡çš„ä¸ªæ€§åŒ–æ¨è
- **æ•°æ®åŒæ­¥**ï¼šé€šè¿‡ Kafka CDC ä» PostgreSQL åŒæ­¥ç»´åº¦æ•°æ®
- **æ€§èƒ½ä¼˜åŒ–**ï¼šP95 æŸ¥è¯¢å»¶è¿Ÿ < 800msï¼ˆ50 æ¡å€™é€‰å¸–å­ï¼‰

---

## ğŸ—ï¸ Architecture

```
PostgreSQL (OLTP)          Kafka (Event Stream)
      |                           |
      |-- CDC (Debezium)          |-- Events Producer
      |                           |
      v                           v
   [Kafka Topics]          [Kafka Topics]
      |                           |
      +----------> ClickHouse <---+
                       |
                       v
          [Materialized Views] â†’ [Aggregation Tables]
                       |
                       v
               [Feed Ranking Queries]
                       |
                       v
                  Rust Backend
```

### Data Flow

1. **OLTP â†’ CDC â†’ Kafka**
   - PostgreSQL è¡¨ï¼ˆposts, follows, likes, commentsï¼‰é€šè¿‡ Debezium CDC åŒæ­¥åˆ° Kafka
   - Kafka topics: `postgres.public.posts`, `postgres.public.follows`, etc.

2. **Event Producers â†’ Kafka**
   - ç”¨æˆ·è¡Œä¸ºäº‹ä»¶ï¼ˆimpression, view, like, comment, shareï¼‰ç”± Rust åç«¯å‘é€åˆ° Kafka
   - Kafka topic: `events`

3. **Kafka â†’ ClickHouse**
   - Kafka Engine è¡¨å®æ—¶æ¶ˆè´¹ Kafka æ•°æ®
   - Materialized Views å°†æ•°æ®è½¬æ¢å¹¶èšåˆåˆ°ç›®æ ‡è¡¨

4. **ClickHouse â†’ Backend**
   - Rust åç«¯é€šè¿‡ HTTP/Native åè®®æŸ¥è¯¢ ClickHouse
   - è·å– Feed æ’åºæ‰€éœ€çš„æŒ‡æ ‡å’Œäº²å’Œåº¦æ•°æ®

---

## ğŸ“ File Structure

```
backend/infra/clickhouse/
â”œâ”€â”€ schema.sql                  # æ ¸å¿ƒè¡¨å®šä¹‰ï¼ˆevents, posts, follows, metricsï¼‰
â”œâ”€â”€ kafka-engines.sql          # Kafka Engine è¡¨é…ç½®ï¼ˆæ¶ˆè´¹è€…è®¾ç½®ï¼‰
â”œâ”€â”€ materialized-views.sql     # ç‰©åŒ–è§†å›¾ï¼ˆå®æ—¶èšåˆ + CDC åŒæ­¥ï¼‰
â”œâ”€â”€ init.sh                    # åˆå§‹åŒ–è„šæœ¬ï¼ˆå¹‚ç­‰ï¼Œå¯é‡å¤æ‰§è¡Œï¼‰
â”œâ”€â”€ docker-compose.yml         # æœ¬åœ°å¼€å‘ç¯å¢ƒï¼ˆClickHouse + Kafka + Zookeeperï¼‰
â”œâ”€â”€ config.xml                 # ClickHouse æœåŠ¡å™¨é…ç½®
â”œâ”€â”€ users.xml                  # ç”¨æˆ·æƒé™é…ç½®
â”œâ”€â”€ queries/
â”‚   â”œâ”€â”€ feed-ranking.sql       # Feed æ’åºæŸ¥è¯¢æ¨¡æ¿ï¼ˆ6 ç§åœºæ™¯ï¼‰
â”‚   â””â”€â”€ test-data.sql          # æµ‹è¯•æ•°æ®ç”Ÿæˆè„šæœ¬
â””â”€â”€ README.md                  # æœ¬æ–‡æ¡£
```

---

## ğŸš€ Quick Start

### 1. æœ¬åœ°å¼€å‘ç¯å¢ƒå¯åŠ¨

```bash
# è¿›å…¥ ClickHouse ç›®å½•
cd backend/infra/clickhouse

# å¯åŠ¨æ‰€æœ‰æœåŠ¡ï¼ˆClickHouse + Kafka + Zookeeperï¼‰
docker-compose up -d

# æ£€æŸ¥æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹ ClickHouse æ—¥å¿—
docker-compose logs -f clickhouse
```

**æœåŠ¡ç«¯å£**ï¼š
- ClickHouse HTTP: `http://localhost:8123`
- ClickHouse Native: `localhost:9000`
- Kafka: `localhost:9092`
- Kafka UI: `http://localhost:8081`
- Zookeeper: `localhost:2181`

### 2. åˆå§‹åŒ–æ•°æ®åº“

```bash
# è‡ªåŠ¨åˆ›å»ºæ‰€æœ‰è¡¨ã€Kafka Engineã€ç‰©åŒ–è§†å›¾
./init.sh

# æˆ–è€…æ‰‹åŠ¨æ‰§è¡Œï¼ˆå¦‚æœåœ¨å®¹å™¨å†…ï¼‰
docker exec -it nova-clickhouse bash
clickhouse-client --multiquery < /schema.sql
clickhouse-client --multiquery < /kafka-engines.sql
clickhouse-client --multiquery < /materialized-views.sql
```

**éªŒè¯åˆå§‹åŒ–**ï¼š
```sql
-- è¿æ¥ ClickHouse
clickhouse-client

-- æŸ¥çœ‹æ‰€æœ‰è¡¨
SHOW TABLES FROM nova_analytics;

-- æŸ¥çœ‹ç‰©åŒ–è§†å›¾
SELECT name, engine FROM system.tables
WHERE database = 'nova_analytics' AND name LIKE 'mv_%';

-- æŸ¥çœ‹ Kafka æ¶ˆè´¹è€…çŠ¶æ€
SELECT * FROM system.kafka_consumers;
```

### 3. åŠ è½½æµ‹è¯•æ•°æ®

```bash
# åŠ è½½ç¤ºä¾‹æ•°æ®ï¼ˆç”¨æˆ·ã€å¸–å­ã€å…³æ³¨ã€äº‹ä»¶ï¼‰
clickhouse-client --multiquery < queries/test-data.sql

# éªŒè¯æ•°æ®
clickhouse-client --query "SELECT count(*) FROM nova_analytics.events"
clickhouse-client --query "SELECT count(*) FROM nova_analytics.posts FINAL"
```

### 4. æµ‹è¯• Feed æŸ¥è¯¢

```bash
# æ‰“å¼€ ClickHouse å®¢æˆ·ç«¯
clickhouse-client

# æ‰§è¡Œä¸ªæ€§åŒ– Feed æŸ¥è¯¢ï¼ˆBob çš„ Feedï¼‰
USE nova_analytics;

SELECT
  p.id AS post_id,
  p.caption,
  sum(pm.likes_count) AS likes,
  sum(pm.comments_count) AS comments
FROM posts AS p FINAL
INNER JOIN follows AS f FINAL
  ON f.following_id = p.user_id
  AND f.follower_id = '22222222-2222-2222-2222-222222222222'
LEFT JOIN post_metrics_1h AS pm
  ON pm.post_id = p.id
WHERE p.status = 'published' AND p.__deleted = 0
GROUP BY p.id, p.caption
ORDER BY p.created_at DESC
LIMIT 10;
```

---

## ğŸ“Š Table Schemas

### æ ¸å¿ƒè¡¨

| è¡¨å | å¼•æ“ | ç”¨é€” | ä¿ç•™æœŸ |
|------|------|------|--------|
| `events` | MergeTree | åŸå§‹äº‹ä»¶æ•°æ® | 90 å¤© |
| `posts` | ReplacingMergeTree | å¸–å­ç»´åº¦è¡¨ï¼ˆCDCï¼‰ | æ°¸ä¹… |
| `follows` | ReplacingMergeTree | å…³æ³¨å…³ç³»ï¼ˆCDCï¼‰ | æ°¸ä¹… |
| `likes` | ReplacingMergeTree | ç‚¹èµè®°å½•ï¼ˆCDCï¼‰ | æ°¸ä¹… |
| `comments` | ReplacingMergeTree | è¯„è®ºæ•°æ®ï¼ˆCDCï¼‰ | æ°¸ä¹… |
| `post_metrics_1h` | SummingMergeTree | å°æ—¶çº§èšåˆæŒ‡æ ‡ | 30 å¤© |
| `user_author_affinity` | ReplacingMergeTree | ç”¨æˆ·-ä½œè€…äº²å’Œåº¦ | 90 å¤© |
| `hot_posts` | ReplacingMergeTree | çƒ­é—¨å¸–å­ç¼“å­˜ | 2 å¤© |

### Kafka Engine è¡¨

| è¡¨å | Topic | æ¶ˆè´¹ç»„ | æ ¼å¼ |
|------|-------|--------|------|
| `events_kafka` | `events` | `clickhouse-consumer-events` | JSONEachRow |
| `posts_kafka` | `postgres.public.posts` | `clickhouse-consumer-posts-cdc` | JSONEachRow |
| `follows_kafka` | `postgres.public.follows` | `clickhouse-consumer-follows-cdc` | JSONEachRow |
| `likes_kafka` | `postgres.public.likes` | `clickhouse-consumer-likes-cdc` | JSONEachRow |
| `comments_kafka` | `postgres.public.comments` | `clickhouse-consumer-comments-cdc` | JSONEachRow |

### ç‰©åŒ–è§†å›¾

| è§†å›¾å | æºè¡¨ | ç›®æ ‡è¡¨ | ä½œç”¨ |
|--------|------|--------|------|
| `mv_events_ingest` | `events_kafka` | `events` | äº‹ä»¶æµæ¶ˆè´¹ |
| `mv_post_metrics_1h` | `events` | `post_metrics_1h` | å°æ—¶èšåˆ |
| `mv_user_author_affinity` | `events` | `user_author_affinity` | äº²å’Œåº¦è®¡ç®— |
| `mv_posts_cdc` | `posts_kafka` | `posts` | CDC åŒæ­¥ |
| `mv_follows_cdc` | `follows_kafka` | `follows` | CDC åŒæ­¥ |
| `mv_likes_cdc` | `likes_kafka` | `likes` | CDC åŒæ­¥ |
| `mv_comments_cdc` | `comments_kafka` | `comments` | CDC åŒæ­¥ |

---

## ğŸ” Query Templates

### 1. ä¸ªæ€§åŒ– Feedï¼ˆå…³æ³¨ç”¨æˆ·ï¼‰

```sql
-- å‚æ•°åŒ–æŸ¥è¯¢ï¼ˆRust åç«¯ä½¿ç”¨ï¼‰
-- {user_id}: å½“å‰ç”¨æˆ· UUID
-- {limit}: è¿”å›æ•°é‡ï¼ˆé»˜è®¤ 50ï¼‰
-- {lookback_hours}: æ—¶é—´çª—å£ï¼ˆé»˜è®¤ 72 å°æ—¶ï¼‰

SELECT
  p.id AS post_id,
  p.user_id AS author_id,
  p.caption,
  p.created_at,
  sum(pm.likes_count) AS likes,
  sum(pm.comments_count) AS comments,
  sum(pm.shares_count) AS shares,

  -- ç»¼åˆå¾—åˆ†ï¼ˆæ–°é²œåº¦ 30% + äº’åŠ¨ 40% + äº²å’Œåº¦ 30%ï¼‰
  round(
    0.30 * exp(-0.10 * dateDiff('hour', p.created_at, now())) +
    0.40 * log1p(sum(pm.likes_count) + 2*sum(pm.comments_count) + 3*sum(pm.shares_count)) / greatest(sum(pm.impressions_count), 1) +
    0.30 * coalesce(ua.interaction_count / 100.0, 0.01),
    4
  ) AS score
FROM posts AS p FINAL
INNER JOIN follows AS f FINAL
  ON f.following_id = p.user_id
  AND f.follower_id = {user_id:UUID}
  AND f.__deleted = 0
LEFT JOIN post_metrics_1h AS pm
  ON pm.post_id = p.id
  AND pm.metric_hour >= toStartOfHour(now()) - INTERVAL 24 HOUR
LEFT JOIN user_author_affinity AS ua
  ON ua.user_id = {user_id:UUID}
  AND ua.author_id = p.user_id
PREWHERE
  p.status = 'published'
  AND p.soft_delete IS NULL
  AND p.__deleted = 0
  AND p.created_at >= now() - INTERVAL {lookback_hours:UInt16} HOUR
GROUP BY p.id, p.user_id, p.caption, p.created_at, ua.interaction_count
ORDER BY score DESC
LIMIT {limit:UInt16};
```

### 2. å‘ç°é¡µï¼ˆçƒ­é—¨å†…å®¹ï¼‰

```sql
-- ä½¿ç”¨é¢„è®¡ç®—çš„çƒ­é—¨å¸–å­è¡¨
SELECT
  post_id,
  author_id,
  score,
  likes,
  comments,
  shares
FROM hot_posts
WHERE collected_at = (SELECT max(collected_at) FROM hot_posts)
ORDER BY score DESC
LIMIT 50;
```

### 3. ä½œè€…ä¸»é¡µ

```sql
-- æŸ¥çœ‹ç‰¹å®šä½œè€…çš„æ‰€æœ‰å¸–å­
SELECT
  p.id AS post_id,
  p.caption,
  p.created_at,
  sum(pm.likes_count) AS likes,
  sum(pm.comments_count) AS comments
FROM posts AS p FINAL
LEFT JOIN post_metrics_1h AS pm
  ON pm.post_id = p.id
PREWHERE
  p.user_id = {author_id:UUID}
  AND p.status = 'published'
  AND p.__deleted = 0
GROUP BY p.id, p.caption, p.created_at
ORDER BY p.created_at DESC
LIMIT 50;
```

æ›´å¤šæŸ¥è¯¢æ¨¡æ¿è¯¦è§ `queries/feed-ranking.sql`ã€‚

---

## âš™ï¸ Configuration

### ç¯å¢ƒå˜é‡

**Docker Compose (`.env` æ–‡ä»¶)**:
```bash
CLICKHOUSE_DB=nova_analytics
CLICKHOUSE_USER=default
CLICKHOUSE_PASSWORD=  # ç•™ç©ºè¡¨ç¤ºæ— å¯†ç ï¼ˆå¼€å‘ç¯å¢ƒï¼‰

KAFKA_BROKER=kafka:9092
KAFKA_GROUP_PREFIX=clickhouse-consumer
```

**ç”Ÿäº§ç¯å¢ƒ**:
```bash
export CLICKHOUSE_HOST=clickhouse.prod.example.com
export CLICKHOUSE_PORT=9000
export CLICKHOUSE_PASSWORD=your_secure_password
export KAFKA_BROKER=kafka1.prod:9093,kafka2.prod:9093,kafka3.prod:9093
export KAFKA_GROUP_PREFIX=clickhouse-prod
```

### æ€§èƒ½è°ƒä¼˜

**ClickHouse é…ç½® (`config.xml`)**:
```xml
<!-- æœ€å¤§å†…å­˜é™åˆ¶ï¼ˆç”Ÿäº§ç¯å¢ƒå»ºè®® 16GB+ï¼‰ -->
<max_server_memory_usage>17179869184</max_server_memory_usage>

<!-- æœ€å¤§å¹¶å‘æŸ¥è¯¢æ•°ï¼ˆæ ¹æ® CPU æ ¸å¿ƒæ•°è°ƒæ•´ï¼‰ -->
<max_concurrent_queries>200</max_concurrent_queries>

<!-- åå°åˆå¹¶çº¿ç¨‹ï¼ˆSSD å»ºè®® 32+ï¼‰ -->
<background_pool_size>32</background_pool_size>
```

**Kafka Engine è°ƒä¼˜**:
```sql
-- é«˜åååœºæ™¯ï¼ˆå¢åŠ æ¶ˆè´¹è€…æ•°é‡ï¼ŒåŒ¹é… Kafka åˆ†åŒºæ•°ï¼‰
kafka_num_consumers = 4  -- å¦‚æœ topic æœ‰ 4 ä¸ªåˆ†åŒº

-- ä½å»¶è¿Ÿåœºæ™¯ï¼ˆå‡å°æ‰¹æ¬¡å¤§å°ï¼‰
kafka_max_block_size = 262144  -- 256KB

-- ä¸¥æ ¼ä¸€è‡´æ€§ï¼ˆç¦æ­¢è·³è¿‡é”™è¯¯æ¶ˆæ¯ï¼‰
kafka_skip_broken_messages = 0
```

---

## ğŸ§ª Testing

### å•å…ƒæµ‹è¯•ï¼ˆClickHouse å†…ç½®ï¼‰

```sql
-- æµ‹è¯•äº‹ä»¶æ¶ˆè´¹é€Ÿç‡
SELECT
  toStartOfMinute(created_at) AS minute,
  count(*) AS events_ingested,
  count() / 60 AS events_per_second
FROM events
WHERE created_at >= now() - INTERVAL 1 HOUR
GROUP BY minute
ORDER BY minute DESC;

-- æµ‹è¯•ç‰©åŒ–è§†å›¾å»¶è¿Ÿ
SELECT
  name,
  total_rows,
  formatReadableSize(total_bytes) AS size,
  max(last_exception_time) AS last_error
FROM system.tables
WHERE database = 'nova_analytics' AND name LIKE 'mv_%'
GROUP BY name, total_rows, total_bytes;

-- æµ‹è¯•æŸ¥è¯¢æ€§èƒ½ï¼ˆå¸¦ EXPLAINï¼‰
EXPLAIN
SELECT * FROM posts FINAL WHERE user_id = '...' LIMIT 10;
```

### é›†æˆæµ‹è¯•ï¼ˆKafka æ•°æ®æµï¼‰

```bash
# 1. å¯åŠ¨ Kafka ç”Ÿäº§è€…
docker exec -it nova-kafka bash

# 2. å‘é€æµ‹è¯•äº‹ä»¶
kafka-console-producer.sh \
  --topic events \
  --bootstrap-server localhost:9092

# ç²˜è´´ JSON æ•°æ®ï¼š
{"event_id":"123e4567-e89b-12d3-a456-426614174000","user_id":"223e4567-e89b-12d3-a456-426614174000","post_id":"323e4567-e89b-12d3-a456-426614174000","event_type":"view","author_id":"423e4567-e89b-12d3-a456-426614174000","dwell_ms":5000,"created_at":"2025-10-18 10:00:00"}

# 3. éªŒè¯ ClickHouse æ¥æ”¶
clickhouse-client --query "SELECT * FROM nova_analytics.events WHERE event_id = '123e4567-e89b-12d3-a456-426614174000'"
```

### è´Ÿè½½æµ‹è¯•

```bash
# ä½¿ç”¨ clickhouse-benchmark å·¥å…·
echo "SELECT * FROM nova_analytics.posts FINAL WHERE status = 'published' LIMIT 50" | \
  clickhouse-benchmark \
    --host=localhost \
    --port=9000 \
    --concurrency=10 \
    --iterations=1000

# æœŸæœ›ç»“æœï¼š
# - P95 < 800ms
# - P99 < 1500ms
# - QPS > 100
```

---

## ğŸ“ˆ Monitoring

### å…³é”®æŒ‡æ ‡

**ç³»ç»Ÿçº§**:
```sql
-- è¡¨å¤§å°ç»Ÿè®¡
SELECT
  table,
  formatReadableSize(sum(bytes)) AS size,
  sum(rows) AS rows,
  count() AS partitions
FROM system.parts
WHERE database = 'nova_analytics' AND active
GROUP BY table
ORDER BY sum(bytes) DESC;

-- åˆ†åŒºæ•°é‡ï¼ˆè¿‡å¤šä¼šå½±å“æ€§èƒ½ï¼‰
SELECT
  table,
  count(DISTINCT partition) AS partition_count,
  min(partition) AS oldest_partition,
  max(partition) AS newest_partition
FROM system.parts
WHERE database = 'nova_analytics' AND active
GROUP BY table;
```

**æŸ¥è¯¢æ€§èƒ½**:
```sql
-- æ…¢æŸ¥è¯¢ï¼ˆP95 > 1sï¼‰
SELECT
  query_duration_ms,
  query,
  read_rows,
  formatReadableSize(read_bytes) AS read_size,
  event_time
FROM system.query_log
WHERE event_date = today()
  AND type = 'QueryFinish'
  AND query_duration_ms > 1000
  AND query NOT LIKE '%system.%'
ORDER BY query_duration_ms DESC
LIMIT 20;
```

**Kafka æ¶ˆè´¹**:
```sql
-- æ¶ˆè´¹è€…çŠ¶æ€æ£€æŸ¥
SELECT
  table,
  consumer_number,
  assignments.topic_name,
  assignments.partition_id,
  assignments.current_offset,
  exceptions.time AS last_error_time,
  exceptions.text AS last_error
FROM system.kafka_consumers
WHERE database = 'nova_analytics';
```

### Grafana Dashboard

æ¨èç›‘æ§æŒ‡æ ‡ï¼ˆä½¿ç”¨ Prometheus + ClickHouse Exporterï¼‰:

1. **äº‹ä»¶æ‘„å…¥é€Ÿç‡**ï¼š`events_per_second`
2. **æŸ¥è¯¢ P95 å»¶è¿Ÿ**ï¼š`query_duration_p95_ms`
3. **Kafka æ¶ˆè´¹å»¶è¿Ÿ**ï¼š`kafka_consumer_lag`
4. **è¡¨å¤§å°å¢é•¿**ï¼š`table_bytes_growth_per_hour`
5. **ç£ç›˜ä½¿ç”¨ç‡**ï¼š`disk_usage_percent`
6. **CPU/å†…å­˜ä½¿ç”¨**ï¼š`cpu_percent`, `memory_usage_bytes`

---

## ğŸ”§ Troubleshooting

### é—®é¢˜ 1: Kafka æ¶ˆè´¹åœæ»

**ç—‡çŠ¶**ï¼š`system.kafka_consumers` æ˜¾ç¤º offset ä¸å¢åŠ 

**æ’æŸ¥æ­¥éª¤**ï¼š
```bash
# 1. æ£€æŸ¥ Kafka broker è¿æ¥
docker exec -it nova-kafka kafka-broker-api-versions.sh --bootstrap-server localhost:9092

# 2. æ£€æŸ¥ topic æ˜¯å¦å­˜åœ¨
docker exec -it nova-kafka kafka-topics.sh --list --bootstrap-server localhost:9092

# 3. æŸ¥çœ‹æ¶ˆè´¹è€…ç»„çŠ¶æ€
docker exec -it nova-kafka kafka-consumer-groups.sh \
  --describe \
  --group clickhouse-consumer-events \
  --bootstrap-server localhost:9092

# 4. é‡ç½® ClickHouse Kafka offsetï¼ˆè°¨æ…æ“ä½œï¼ï¼‰
clickhouse-client --query "DROP TABLE nova_analytics.events_kafka"
clickhouse-client --multiquery < kafka-engines.sql
```

### é—®é¢˜ 2: æŸ¥è¯¢æ€§èƒ½æ…¢

**ç—‡çŠ¶**ï¼šFeed æŸ¥è¯¢ P95 > 2s

**ä¼˜åŒ–æ–¹æ¡ˆ**ï¼š
```sql
-- 1. æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† FINALï¼ˆå¼ºåˆ¶å»é‡ï¼Œå¾ˆæ…¢ï¼‰
-- è§£å†³ï¼šå®šæœŸæ‰§è¡Œ OPTIMIZE TABLE é¢„åˆå¹¶æ•°æ®
OPTIMIZE TABLE posts FINAL;
OPTIMIZE TABLE follows FINAL;

-- 2. æ£€æŸ¥ ORDER BY æ˜¯å¦åˆ©ç”¨äº†ä¸»é”®ç´¢å¼•
-- è§£å†³ï¼šç¡®ä¿æŸ¥è¯¢çš„ ORDER BY ä¸è¡¨çš„ ORDER BY ä¸€è‡´

-- 3. æ£€æŸ¥æ˜¯å¦æ‰«æäº†è¿‡å¤šæ•°æ®
-- è§£å†³ï¼šä½¿ç”¨ PREWHERE è¿‡æ»¤ï¼ˆæ¯” WHERE æ›´é«˜æ•ˆï¼‰
SELECT * FROM posts
PREWHERE status = 'published' AND created_at >= now() - INTERVAL 72 HOUR
WHERE user_id IN (SELECT following_id FROM follows WHERE follower_id = '...');

-- 4. æ£€æŸ¥ JOIN æ˜¯å¦äº§ç”Ÿäº†ç¬›å¡å°”ç§¯
-- è§£å†³ï¼šç¡®ä¿ JOIN æ¡ä»¶åŒ…å«é«˜é€‰æ‹©æ€§å­—æ®µï¼ˆå¦‚ UUIDï¼‰
```

### é—®é¢˜ 3: ç£ç›˜ç©ºé—´ä¸è¶³

**ç—‡çŠ¶**ï¼š`Disk is almost full` é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**ï¼š
```sql
-- 1. æ‰‹åŠ¨æ¸…ç†è¿‡æœŸåˆ†åŒºï¼ˆTTL æœªè‡ªåŠ¨è§¦å‘ï¼‰
ALTER TABLE events DROP PARTITION '202509';  -- åˆ é™¤ 2025 å¹´ 9 æœˆæ•°æ®

-- 2. å¼ºåˆ¶æ‰§è¡Œ TTL åˆå¹¶
OPTIMIZE TABLE events FINAL;

-- 3. æ£€æŸ¥å“ªäº›è¡¨å ç”¨ç©ºé—´æœ€å¤š
SELECT
  table,
  formatReadableSize(sum(bytes)) AS size,
  sum(rows) AS rows
FROM system.parts
WHERE database = 'nova_analytics' AND active
GROUP BY table
ORDER BY sum(bytes) DESC;

-- 4. å‹ç¼©å†å²æ•°æ®ï¼ˆé™ä½å­˜å‚¨æˆæœ¬ï¼‰
ALTER TABLE events MODIFY SETTING storage_policy = 'cold_storage';
```

### é—®é¢˜ 4: ç‰©åŒ–è§†å›¾æ•°æ®ä¸ä¸€è‡´

**ç—‡çŠ¶**ï¼š`post_metrics_1h` çš„æ•°æ®ä¸ `events` ä¸åŒ¹é…

**åŸå› **ï¼š
- ç‰©åŒ–è§†å›¾åªå¤„ç†åˆ›å»ºåçš„æ–°æ•°æ®
- å†å²æ•°æ®éœ€è¦æ‰‹åŠ¨å›å¡«

**è§£å†³æ–¹æ¡ˆ**ï¼š
```sql
-- å›å¡«å†å²æ•°æ®ï¼ˆä¸€æ¬¡æ€§æ“ä½œï¼‰
INSERT INTO post_metrics_1h
SELECT
  post_id,
  author_id,
  toStartOfHour(created_at) AS metric_hour,
  sumIf(1, event_type = 'like') AS likes_count,
  sumIf(1, event_type = 'comment') AS comments_count,
  sumIf(1, event_type = 'share') AS shares_count,
  sumIf(1, event_type = 'impression') AS impressions_count,
  sumIf(1, event_type = 'view') AS views_count,
  avgIf(dwell_ms, event_type IN ('view', 'impression') AND dwell_ms IS NOT NULL) AS avg_dwell_ms,
  uniqState(user_id) AS unique_viewers,
  now() AS updated_at
FROM events
WHERE post_id IS NOT NULL
  AND author_id IS NOT NULL
  AND created_at >= '2025-01-01' AND created_at < now()
GROUP BY post_id, author_id, metric_hour;
```

---

## ğŸš€ Production Deployment

### éƒ¨ç½²æ¸…å•

- [ ] ä¿®æ”¹ `KAFKA_BROKER` ä¸ºç”Ÿäº§ Kafka é›†ç¾¤åœ°å€
- [ ] è®¾ç½®å¼ºå¯†ç ï¼ˆ`CLICKHOUSE_PASSWORD`ï¼‰
- [ ] é…ç½®ç”Ÿäº§çº§èµ„æºé™åˆ¶ï¼ˆ`config.xml` ä¸­çš„å†…å­˜/CPUï¼‰
- [ ] å¯ç”¨ TLS åŠ å¯†ï¼ˆKafka å’Œ ClickHouseï¼‰
- [ ] é…ç½® ACL æƒé™æ§åˆ¶ï¼ˆé™åˆ¶ç”¨æˆ·è®¿é—®ï¼‰
- [ ] è®¾ç½® Kafka topic åˆ†åŒºæ•°ï¼ˆå»ºè®® 8-16 åˆ†åŒºï¼‰
- [ ] é…ç½®ç›‘æ§å‘Šè­¦ï¼ˆGrafana + Prometheusï¼‰
- [ ] æµ‹è¯•ç¾éš¾æ¢å¤æµç¨‹ï¼ˆå¤‡ä»½ + æ¢å¤ï¼‰
- [ ] è®¾ç½®è‡ªåŠ¨å¤‡ä»½ç­–ç•¥ï¼ˆæ¯æ—¥å…¨é‡ + å°æ—¶å¢é‡ï¼‰
- [ ] å‹åŠ›æµ‹è¯•ï¼ˆ10K events/s + 100 QPS æŸ¥è¯¢ï¼‰

### Kubernetes éƒ¨ç½²ï¼ˆå¯é€‰ï¼‰

```yaml
# clickhouse-statefulset.yamlï¼ˆç¤ºä¾‹ï¼‰
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: clickhouse
spec:
  serviceName: clickhouse
  replicas: 3  # é«˜å¯ç”¨é›†ç¾¤
  selector:
    matchLabels:
      app: clickhouse
  template:
    metadata:
      labels:
        app: clickhouse
    spec:
      containers:
      - name: clickhouse
        image: clickhouse/clickhouse-server:23.8
        ports:
        - containerPort: 9000
          name: native
        - containerPort: 8123
          name: http
        volumeMounts:
        - name: clickhouse-data
          mountPath: /var/lib/clickhouse
        env:
        - name: CLICKHOUSE_PASSWORD
          valueFrom:
            secretKeyRef:
              name: clickhouse-secret
              key: password
  volumeClaimTemplates:
  - metadata:
      name: clickhouse-data
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 500Gi  # SSD æ¨è
```

---

## ğŸ“š References

- [ClickHouse Official Documentation](https://clickhouse.com/docs/)
- [Kafka Engine Documentation](https://clickhouse.com/docs/en/engines/table-engines/integrations/kafka)
- [Materialized Views Guide](https://clickhouse.com/docs/en/guides/developer/cascading-materialized-views)
- [ReplacingMergeTree](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replacingmergetree)
- [SummingMergeTree](https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/summingmergetree)
- [Query Optimization](https://clickhouse.com/docs/en/guides/developer/optimize-query-performance)

---

## ğŸ“ Changelog

### v1.0.0 (2025-10-18)

- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒ 7 å¼ æ ¸å¿ƒè¡¨ï¼ˆevents, posts, follows, likes, comments, metrics, affinityï¼‰
- 5 ä¸ª Kafka Engine è¡¨ï¼ˆäº‹ä»¶æµ + CDC åŒæ­¥ï¼‰
- 7 ä¸ªç‰©åŒ–è§†å›¾ï¼ˆå®æ—¶èšåˆ + ç»´åº¦åŒæ­¥ï¼‰
- 6 ä¸ªæŸ¥è¯¢æ¨¡æ¿ï¼ˆä¸ªæ€§åŒ– Feedã€å‘ç°é¡µã€ä½œè€…ä¸»é¡µã€æ‰¹é‡æŒ‡æ ‡ã€äº²å’Œåº¦ã€çƒ­æ¦œï¼‰
- Docker Compose å¼€å‘ç¯å¢ƒ
- å®Œæ•´çš„æµ‹è¯•æ•°æ®å’ŒéªŒè¯è„šæœ¬

---

## ğŸ¤ Contributing

å¦‚æœä½ å‘ç° bug æˆ–æœ‰æ”¹è¿›å»ºè®®ï¼š

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ (`git checkout -b feature/optimization`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Optimize feed ranking query'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/optimization`)
5. åˆ›å»º Pull Request

---

## ğŸ“„ License

MIT License - è¯¦è§ LICENSE æ–‡ä»¶

---

**ç»´æŠ¤è€…**: Nova Backend Team
**æœ€åæ›´æ–°**: 2025-10-18
**çŠ¶æ€**: ç”Ÿäº§å°±ç»ª âœ…

# Phase 3 Implementation Guide: å¯¦æ™‚å€‹æ€§åŒ– Feed æ’åºç³»çµ±

**åŸºäºç°æœ‰ä»£ç åˆ†æ** | **ä¸ Nova Backend å…¼å®¹** | **åˆ†é˜¶æ®µæŒ‡å¯¼**

---

## ğŸ“‹ ç›®å½•

1. [æ•´ä½“ç°çŠ¶](#æ•´ä½“ç°çŠ¶)
2. [å…³é”®é˜»å¡é¡¹](#å…³é”®é˜»å¡é¡¹)
3. [åˆ†é˜¶æ®µå®æ–½è®¡åˆ’](#åˆ†é˜¶æ®µå®æ–½è®¡åˆ’)
4. [Phase 1 è¯¦ç»†æŒ‡å—ï¼šåŸºç¡€è®¾æ–½](#phase-1-è¯¦ç»†æŒ‡å—åŸºç¡€è®¾æ–½)
5. [ä»£ç é›†æˆç‚¹](#ä»£ç é›†æˆç‚¹)
6. [æµ‹è¯•ç­–ç•¥](#æµ‹è¯•ç­–ç•¥)
7. [éƒ¨ç½²æ¸…å•](#éƒ¨ç½²æ¸…å•)

---

## æ•´ä½“ç°çŠ¶

### âœ… å·²æœ‰ç»„ä»¶ï¼ˆ60%ï¼‰

```
âœ“ ClickHouse å®¢æˆ·ç«¯ + è¿æ¥æ±  (src/db/ch_client.rs)
âœ“ Redis ç¼“å­˜ç³»ç»Ÿ (src/cache/feed_cache.rs)
âœ“ Feed æ’åºæœåŠ¡ (src/services/feed_ranking.rs)
âœ“ Feed Handler API (src/handlers/feed.rs)
âœ“ è¶‹åŠ¿ Job (src/jobs/trending_generator.rs)
âœ“ å»ºè®®ç”¨æˆ· Job (src/jobs/suggested_users_generator.rs)
âœ“ Events Handler åŸºç¡€ (src/handlers/events.rs)
âœ“ Kafka ç”Ÿäº§è€… (src/services/kafka/producer.rs)
âœ“ JWT è®¤è¯ã€é€Ÿç‡é™åˆ¶ã€æŒ‡æ ‡ç³»ç»Ÿ
```

### âŒ å…³é”®ç¼ºå¤±ï¼ˆ40%ï¼‰

```
âœ— CDC æ¶ˆè´¹è€…æœåŠ¡ï¼ˆPostgreSQL â†’ Kafka â†’ ClickHouseï¼‰
âœ— Events æ¶ˆè´¹è€…æœåŠ¡ï¼ˆEvents â†’ ClickHouseï¼‰
âœ— äº‹ä»¶å»é‡é€»è¾‘
âœ— ClickHouse ç‰©åŒ–è§†å›¾
âœ— Circuit Breaker æ¨¡å¼
âœ— å®æ—¶ç¼“å­˜å¤±æ•ˆ
âœ— CDC ç®¡é“æŒ‡æ ‡
```

---

## å…³é”®é˜»å¡é¡¹

### ğŸ”´ P0 - CRITICALï¼ˆå¿…é¡»è§£å†³æ‰èƒ½å¯åŠ¨ Phase 3ï¼‰

#### 1. CDC æ¶ˆè´¹è€…æœåŠ¡ï¼ˆ3-5 å¤©å·¥ä½œé‡ï¼‰

**å½“å‰é—®é¢˜**ï¼š
```
PostgreSQL (posts/likes/follows)
  â†“ Debezium CDC (configured åœ¨ infra/)
  â†“ Kafka topics: cdc.posts, cdc.likes, ...
  âœ— [NOBODY CONSUMES] â† æ•°æ®ä¸¢å¤±ï¼
  â†“ ClickHouse (empty)
```

**éœ€è¦å®ç°**ï¼š
- `src/services/cdc_consumer.rs` - ä» Kafka CDC topics æ¶ˆè´¹
- Offset ç®¡ç†ï¼ˆç¡®ä¿ä¸ä¸¢å¤±ï¼‰
- æ•°æ®éªŒè¯å’Œè½¬æ¢
- é”™è¯¯é‡è¯•å’Œæ­»ä¿¡é˜Ÿåˆ—

**æ–‡ä»¶**ï¼š
```
src/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ cdc/
â”‚   â”‚   â”œâ”€â”€ consumer.rs         (NEW - 200 LOC)
â”‚   â”‚   â”œâ”€â”€ offset_manager.rs   (NEW - 150 LOC)
â”‚   â”‚   â””â”€â”€ models.rs           (NEW - 100 LOC)
â”‚   â””â”€â”€ kafka/
â”‚       â””â”€â”€ consumer.rs         (REFACTOR - exists but needs work)
â””â”€â”€ db/
    â””â”€â”€ cdc_repo.rs             (NEW - 200 LOC)
```

**å…³é”®ä¾èµ–**ï¼š
- `rdkafka` (å·²åœ¨ Cargo.toml)
- `clickhouse` (å·²åœ¨ Cargo.toml)
- PostgreSQL + Debezium (åŸºç¡€è®¾æ–½)

---

#### 2. Events æ¶ˆè´¹è€…æœåŠ¡ï¼ˆ2-3 å¤©å·¥ä½œé‡ï¼‰

**å½“å‰é—®é¢˜**ï¼š
```
POST /api/v1/events
  â†“ src/handlers/events.rs (already implemented)
  â†“ Kafka producer (already sends to "events" topic)
  âœ— [NOBODY CONSUMES] â† äº‹ä»¶ä¸¢å¤±ï¼
  â†“ ClickHouse events table (empty)
```

**éœ€è¦å®ç°**ï¼š
- `src/services/events_consumer.rs` - ä» Kafka "events" topic æ¶ˆè´¹
- äº‹ä»¶å»é‡ï¼ˆä½¿ç”¨ Redis/PostgreSQLï¼‰
- ClickHouse æ’å…¥
- é”™è¯¯å¤„ç†å’Œé‡è¯•

**æ–‡ä»¶**ï¼š
```
src/
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ events/
â”‚   â”‚   â”œâ”€â”€ consumer.rs         (NEW - 250 LOC)
â”‚   â”‚   â”œâ”€â”€ dedup.rs            (NEW - 150 LOC)
â”‚   â”‚   â””â”€â”€ models.rs           (NEW - 100 LOC)
â”‚   â””â”€â”€ kafka/
â”‚       â””â”€â”€ consumer.rs         (CREATE/REFACTOR)
â””â”€â”€ db/
    â””â”€â”€ events_repo.rs          (NEW - 150 LOC)
```

---

### ğŸŸ¡ P1 - HIGHï¼ˆé˜»å¡å¤§éƒ¨åˆ† Phase 3 åŠŸèƒ½ï¼‰

#### 3. ClickHouse ç‰©åŒ–è§†å›¾

**å½“å‰é—®é¢˜**ï¼š
```
Events è¡¨æœ‰ 10M+ è¡Œï¼ˆæœªæ¥ä¼šæ›´å¤šï¼‰
æŸ¥è¯¢æ—¶ï¼šæ²¡æœ‰é¢„èšåˆ â†’ æ¯æ¬¡æŸ¥è¯¢æ‰«æå…¨è¡¨ â†’ æ…¢ï¼
```

**éœ€è¦å®ç°**ï¼š
- `events` â†’ `post_metrics_1h` (æ¯å°æ—¶èšåˆ)
- `events` â†’ `user_author_90d` (ç”¨æˆ·-ä½œè€… 90 å¤©äº²å’Œåº¦)
- ç‰©åŒ–è§†å›¾è‡ªåŠ¨ç»´æŠ¤èšåˆ

**æ–‡ä»¶**ï¼š
```
infra/
â””â”€â”€ clickhouse/
    â”œâ”€â”€ views/
    â”‚   â”œâ”€â”€ mv_post_metrics_1h.sql      (NEW)
    â”‚   â”œâ”€â”€ mv_user_author_90d.sql      (NEW)
    â”‚   â””â”€â”€ mv_post_metrics_daily.sql   (NEW)
    â””â”€â”€ tables/
        â”œâ”€â”€ events.sql                   (UPDATE - add MV config)
        â””â”€â”€ post_metrics_1h.sql          (UPDATE)
```

**å½±å“**ï¼š
- æ²¡æœ‰è¿™äº›ï¼ŒQuery æ—¶é—´ï¼š3-5s (ä¸ç¬¦åˆ â‰¤800ms SLO)
- æœ‰è¿™äº›ï¼ŒQuery æ—¶é—´ï¼š200-300ms âœ“

---

#### 4. Circuit Breaker æ¨¡å¼

**å½“å‰é—®é¢˜**ï¼š
```
å¦‚æœ ClickHouse æ•…éšœ â†’ æ‰€æœ‰æŸ¥è¯¢å¤±è´¥ â†’ Feed å®Œå…¨ä¸å¯ç”¨
åº”è¯¥ï¼šè‡ªåŠ¨å›é€€åˆ° PostgreSQL æ—¶åºæµ
```

**éœ€è¦å®ç°**ï¼š
- `src/middleware/circuit_breaker.rs` (NEW - 200 LOC)
- ä¿®æ”¹ `src/services/feed_ranking.rs` ä½¿ç”¨ Circuit Breaker

**æ–‡ä»¶**ï¼š
```
src/
â”œâ”€â”€ middleware/
â”‚   â””â”€â”€ circuit_breaker.rs      (NEW - 200 LOC)
â””â”€â”€ services/
    â””â”€â”€ feed_ranking.rs         (REFACTOR - add CB logic)
```

---

## åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

### æ—¶é—´è¡¨

```
Week 1 (Mon-Fri): Phase 1 - Foundation
  â”œâ”€ Mon: CDC æ¶ˆè´¹è€…åŸºç¡€ + Offset ç®¡ç†
  â”œâ”€ Tue: CDC æ¶ˆè´¹è€…å®Œæ•´ + æµ‹è¯•
  â”œâ”€ Wed: Events æ¶ˆè´¹è€…åŸºç¡€
  â”œâ”€ Thu: Events æ¶ˆè´¹è€…å®Œæ•´ + å»é‡
  â””â”€ Fri: é›†æˆæµ‹è¯• + ä¿®å¤

Week 2 (Mon-Fri): Phase 2 - Core Features
  â”œâ”€ Mon: ClickHouse ç‰©åŒ–è§†å›¾
  â”œâ”€ Tue: Circuit Breaker å®ç°
  â”œâ”€ Wed: å®æ—¶ç¼“å­˜å¤±æ•ˆ
  â”œâ”€ Thu: æŒ‡æ ‡æ”¶é›† (15+ æ–°æŒ‡æ ‡)
  â””â”€ Fri: ç«¯åˆ°ç«¯æµ‹è¯•

Week 3 (Mon-Fri): Phase 3 - Optimization
  â”œâ”€ Mon-Tue: æ€§èƒ½ä¼˜åŒ– (query profiling, indexing)
  â”œâ”€ Wed-Thu: å‹åŠ›æµ‹è¯• (1k RPS, event-to-visible â‰¤5s)
  â””â”€ Fri: æ–‡æ¡£ + éƒ¨ç½²å‡†å¤‡

æ€»è®¡ï¼š15 å·¥ä½œæ—¥ (2 äººå›¢é˜Ÿ 7.5 å¤©)
```

---

## Phase 1 è¯¦ç»†æŒ‡å—ï¼šåŸºç¡€è®¾æ–½

### Step 1.1: CDC æ¶ˆè´¹è€…æœåŠ¡ï¼ˆ3 å¤©ï¼‰

#### ç›®æ ‡
- ä» Kafka æ¶ˆè´¹ CDC å˜æ›´ï¼ˆposts, follows, comments, likesï¼‰
- æ­£ç¡®ç®¡ç† Offsetï¼ˆä¸ä¸¢å¤±æ•°æ®ï¼‰
- å°†æ•°æ®æ’å…¥ ClickHouse CDC è¡¨

#### 1.1.1 åˆ›å»º CdcMessage æ¨¡å‹

**æ–‡ä»¶**: `src/services/cdc/models.rs`

```rust
use serde::{Deserialize, Serialize};
use chrono::{DateTime, Utc};

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CdcMessage {
    pub table: String,                    // "posts", "follows", etc.
    pub op: CdcOperation,                 // INSERT, UPDATE, DELETE
    pub ts_ms: i64,                       // timestamp in ms
    pub before: Option<serde_json::Value>,
    pub after: serde_json::Value,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq)]
pub enum CdcOperation {
    #[serde(rename = "c")]
    Insert,
    #[serde(rename = "u")]
    Update,
    #[serde(rename = "d")]
    Delete,
    #[serde(rename = "r")]
    Read,
}

impl CdcMessage {
    pub fn validate(&self) -> Result<(), String> {
        // Validate required fields
        if self.table.is_empty() {
            return Err("table is required".to_string());
        }
        if self.after.is_null() {
            return Err("after is required".to_string());
        }
        Ok(())
    }
}
```

**æ£€æŸ¥ç‚¹**:
- [ ] ç¼–è¯‘é€šè¿‡ï¼ˆcargo checkï¼‰
- [ ] åœ¨ `src/services/mod.rs` ä¸­ pub mod cdc

---

#### 1.1.2 åˆ›å»º Offset ç®¡ç†å™¨

**æ–‡ä»¶**: `src/services/cdc/offset_manager.rs`

```rust
use std::sync::Arc;
use sqlx::PgPool;
use tracing::{info, error};
use crate::error::Result;

pub struct OffsetManager {
    db: Arc<PgPool>,
}

impl OffsetManager {
    pub fn new(db: Arc<PgPool>) -> Self {
        Self { db }
    }

    /// åˆ›å»º offset è¡¨ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    pub async fn initialize(&self) -> Result<()> {
        sqlx::query(
            r#"
            CREATE TABLE IF NOT EXISTS cdc_offsets (
                topic TEXT NOT NULL,
                partition INT NOT NULL,
                "offset" BIGINT NOT NULL,
                updated_at TIMESTAMP DEFAULT NOW(),
                PRIMARY KEY (topic, partition)
            )
            "#
        )
        .execute(self.db.as_ref())
        .await?;
        Ok(())
    }

    /// ä¿å­˜ offset
    pub async fn save_offset(&self, topic: &str, partition: i32, offset: i64) -> Result<()> {
        sqlx::query(
            r#"
            INSERT INTO cdc_offsets (topic, partition, "offset", updated_at)
            VALUES ($1, $2, $3, NOW())
            ON CONFLICT (topic, partition) DO UPDATE
            SET "offset" = $3, updated_at = NOW()
            "#
        )
        .bind(topic)
        .bind(partition)
        .bind(offset)
        .execute(self.db.as_ref())
        .await?;
        Ok(())
    }

    /// è¯»å–æœ€åä¿å­˜çš„ offset
    pub async fn read_offset(&self, topic: &str, partition: i32) -> Result<Option<i64>> {
        let row = sqlx::query_scalar::<_, i64>(
            "SELECT \"offset\" FROM cdc_offsets WHERE topic = $1 AND partition = $2"
        )
        .bind(topic)
        .bind(partition)
        .fetch_optional(self.db.as_ref())
        .await?;
        Ok(row)
    }
}
```

**æ£€æŸ¥ç‚¹**:
- [ ] åœ¨æ•°æ®åº“ä¸­åˆ›å»º cdc_offsets è¡¨
- [ ] å¯ä»¥ä¿å­˜å’Œè¯»å– offset

---

#### 1.1.3 åˆ›å»º CDC æ¶ˆè´¹è€…

**æ–‡ä»¶**: `src/services/cdc/consumer.rs` (ä¸»è¦å®ç°)

```rust
use rdkafka::consumer::{Consumer, StreamConsumer};
use rdkafka::ClientConfig;
use std::sync::Arc;
use tracing::{info, error, warn, debug};
use sqlx::PgPool;
use crate::db::ClickHouseClient;
use crate::error::Result;
use super::{models::CdcMessage, offset_manager::OffsetManager};

pub struct CdcConsumer {
    kafka_consumer: Arc<StreamConsumer>,
    offset_manager: Arc<OffsetManager>,
    ch_client: Arc<ClickHouseClient>,
    db: Arc<PgPool>,
}

impl CdcConsumer {
    pub async fn new(
        brokers: &str,
        db: Arc<PgPool>,
        ch_client: Arc<ClickHouseClient>,
    ) -> Result<Self> {
        // Initialize offset manager
        let offset_mgr = Arc::new(OffsetManager::new(db.clone()));
        offset_mgr.initialize().await?;

        // Create Kafka consumer
        let consumer: StreamConsumer = ClientConfig::new()
            .set("bootstrap.servers", brokers)
            .set("group.id", "nova-cdc-consumer-v1")
            .set("auto.offset.reset", "earliest")
            .set("enable.auto.commit", "false")  // Manual commit
            .set("session.timeout.ms", "6000")
            .create()?;

        // Subscribe to CDC topics
        consumer.subscribe(&[
            "cdc.posts",
            "cdc.follows",
            "cdc.comments",
            "cdc.likes",
        ])?;

        Ok(Self {
            kafka_consumer: Arc::new(consumer),
            offset_manager: offset_mgr,
            ch_client,
            db,
        })
    }

    /// Start consuming CDC messages
    pub async fn run(&self) -> Result<()> {
        info!("Starting CDC consumer...");

        loop {
            match self.kafka_consumer.poll(std::time::Duration::from_secs(1)) {
                Some(Ok(msg)) => {
                    if let Err(e) = self.process_message(&msg).await {
                        error!("Failed to process CDC message: {}", e);
                        // Don't commit offset on error, will retry
                    } else {
                        // Successfully processed, commit offset
                        if let Err(e) = self.kafka_consumer.commit_message(&msg, false) {
                            warn!("Failed to commit offset: {}", e);
                        }
                    }
                },
                Some(Err(e)) => {
                    error!("Kafka error: {}", e);
                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                },
                None => {
                    debug!("No message in poll interval");
                }
            }
        }
    }

    /// Process individual CDC message
    async fn process_message(&self, msg: &rdkafka::message::BorrowedMessage) -> Result<()> {
        let payload = msg.payload()
            .ok_or("Message has no payload")?;
        let payload_str = String::from_utf8(payload.to_vec())?;

        let cdc_msg: CdcMessage = serde_json::from_str(&payload_str)?;
        cdc_msg.validate()?;

        debug!("Processing CDC message: table={}, op={:?}", cdc_msg.table, cdc_msg.op);

        // Insert into ClickHouse
        match cdc_msg.table.as_str() {
            "posts" => self.insert_post_cdc(&cdc_msg).await?,
            "follows" => self.insert_follows_cdc(&cdc_msg).await?,
            "comments" => self.insert_comments_cdc(&cdc_msg).await?,
            "likes" => self.insert_likes_cdc(&cdc_msg).await?,
            _ => {
                warn!("Unknown CDC table: {}", cdc_msg.table);
            }
        }

        Ok(())
    }

    /// Insert posts CDC record
    async fn insert_post_cdc(&self, msg: &CdcMessage) -> Result<()> {
        // Extract fields from msg.after
        let query = r#"
            INSERT INTO posts_cdc (post_id, user_id, created_at, deleted, _version)
            VALUES
        "#;

        // TODO: Implement with proper field extraction
        // For now, use a helper method to extract values from JSON

        Ok(())
    }

    // ... Similar methods for follows, comments, likes
}
```

**æ£€æŸ¥ç‚¹**:
- [ ] Kafka consumer è¿æ¥æˆåŠŸ
- [ ] å¯ä»¥è¯»å– CDC messages
- [ ] Offset ç®¡ç†å·¥ä½œæ­£ç¡®

---

#### 1.1.4 åœ¨ä¸»æœåŠ¡ä¸­é›†æˆ CDC æ¶ˆè´¹è€…

**æ–‡ä»¶**: `src/main.rs` - æ·»åŠ  Job å¯åŠ¨é€»è¾‘

```rust
// åœ¨ main() ä¸­æ·»åŠ ï¼š

// Start CDC consumer as background task
let db_clone = db_pool.clone();
let ch_clone = ch_client.clone();
let broker_config = config.kafka_brokers.clone();

tokio::spawn(async move {
    match services::cdc::consumer::CdcConsumer::new(
        &broker_config,
        Arc::new(db_clone),
        Arc::new(ch_clone),
    ).await {
        Ok(consumer) => {
            if let Err(e) = consumer.run().await {
                error!("CDC consumer error: {}", e);
            }
        }
        Err(e) => {
            error!("Failed to initialize CDC consumer: {}", e);
        }
    }
});

info!("CDC consumer started");
```

**æ£€æŸ¥ç‚¹**:
- [ ] æœåŠ¡å™¨å¯åŠ¨æ—¶ CDC consumer ä¹Ÿå¯åŠ¨
- [ ] æ—¥å¿—æ˜¾ç¤º "CDC consumer started"

---

### Step 1.2: Events æ¶ˆè´¹è€…æœåŠ¡ï¼ˆ2 å¤©ï¼‰

#### ç›®æ ‡
- ä» Kafka "events" topic æ¶ˆè´¹
- å®ç°å»é‡ï¼ˆç›¸åŒ event_id ä¸é‡å¤æ’å…¥ï¼‰
- æ’å…¥ ClickHouse events è¡¨

#### 1.2.1 åˆ›å»º Events å»é‡å™¨

**æ–‡ä»¶**: `src/services/events/dedup.rs`

```rust
use std::sync::Arc;
use redis::aio::Connection;
use tracing::debug;
use crate::error::Result;

pub struct EventDeduplicator {
    redis_conn: Arc<tokio::sync::Mutex<Connection>>,
}

impl EventDeduplicator {
    pub fn new(redis_conn: Arc<tokio::sync::Mutex<Connection>>) -> Self {
        Self { redis_conn }
    }

    /// Check if event was already processed (within last 1 hour)
    pub async fn is_duplicate(&self, event_id: &str) -> Result<bool> {
        let key = format!("events:dedup:{}", event_id);
        let mut conn = self.redis_conn.lock().await;

        let exists: bool = redis::cmd("EXISTS")
            .arg(&key)
            .query_async(&mut *conn)
            .await?;

        Ok(exists)
    }

    /// Mark event as processed (TTL 1 hour)
    pub async fn mark_processed(&self, event_id: &str) -> Result<()> {
        let key = format!("events:dedup:{}", event_id);
        let mut conn = self.redis_conn.lock().await;

        redis::cmd("SETEX")
            .arg(&key)
            .arg(3600)  // 1 hour TTL
            .arg("1")
            .query_async(&mut *conn)
            .await?;

        Ok(())
    }
}
```

---

#### 1.2.2 åˆ›å»º Events æ¶ˆè´¹è€…

**æ–‡ä»¶**: `src/services/events/consumer.rs`

```rust
use rdkafka::consumer::{Consumer, StreamConsumer};
use rdkafka::ClientConfig;
use std::sync::Arc;
use tracing::{info, error, debug};
use crate::db::ClickHouseClient;
use crate::error::Result;
use super::dedup::EventDeduplicator;

pub struct EventsConsumer {
    kafka_consumer: Arc<StreamConsumer>,
    ch_client: Arc<ClickHouseClient>,
    dedup: Arc<EventDeduplicator>,
}

impl EventsConsumer {
    pub async fn new(
        brokers: &str,
        ch_client: Arc<ClickHouseClient>,
        dedup: Arc<EventDeduplicator>,
    ) -> Result<Self> {
        let consumer: StreamConsumer = ClientConfig::new()
            .set("bootstrap.servers", brokers)
            .set("group.id", "nova-events-consumer-v1")
            .set("auto.offset.reset", "earliest")
            .set("enable.auto.commit", "true")  // Auto-commit OK for events (idempotent)
            .create()?;

        consumer.subscribe(&["events"])?;

        Ok(Self {
            kafka_consumer: Arc::new(consumer),
            ch_client,
            dedup,
        })
    }

    pub async fn run(&self) -> Result<()> {
        info!("Starting Events consumer...");

        loop {
            match self.kafka_consumer.poll(std::time::Duration::from_secs(1)) {
                Some(Ok(msg)) => {
                    if let Err(e) = self.process_message(&msg).await {
                        error!("Failed to process event: {}", e);
                    }
                },
                Some(Err(e)) => {
                    error!("Kafka error: {}", e);
                    tokio::time::sleep(std::time::Duration::from_secs(1)).await;
                },
                None => {
                    debug!("No message in poll interval");
                }
            }
        }
    }

    async fn process_message(&self, msg: &rdkafka::message::BorrowedMessage) -> Result<()> {
        let payload = msg.payload()
            .ok_or("Message has no payload")?;
        let payload_str = String::from_utf8(payload.to_vec())?;

        let event: serde_json::Value = serde_json::from_str(&payload_str)?;
        let event_id = event["event_id"].as_str()
            .ok_or("Missing event_id")?;

        // Deduplication
        if self.dedup.is_duplicate(event_id).await? {
            debug!("Skipping duplicate event: {}", event_id);
            return Ok(());
        }

        // Insert into ClickHouse
        self.ch_client.insert_event(&event).await?;

        // Mark as processed
        self.dedup.mark_processed(event_id).await?;

        Ok(())
    }
}
```

---

#### 1.2.3 æ›´æ–° ClickHouseClient ä»¥æ”¯æŒ insert_event

**æ–‡ä»¶**: `src/db/ch_client.rs` - æ·»åŠ æ–¹æ³•

```rust
impl ClickHouseClient {
    pub async fn insert_event(&self, event: &serde_json::Value) -> Result<()> {
        let query = r#"
            INSERT INTO events
            (event_id, event_time, user_id, post_id, author_id, action, dwell_ms, device, app_ver)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        "#;

        // Extract fields and insert
        // TODO: implement with proper error handling

        Ok(())
    }
}
```

---

### Step 1.3: é›†æˆå’Œæµ‹è¯•

#### 1.3.1 åˆ›å»ºé›†æˆæµ‹è¯•

**æ–‡ä»¶**: `tests/integration/cdc_events_pipeline_test.rs`

```rust
#[tokio::test]
async fn test_full_cdc_events_pipeline() {
    // 1. Setup: Start Kafka, PostgreSQL, ClickHouse
    // 2. Insert record in PostgreSQL
    // 3. Wait for Debezium to publish to Kafka
    // 4. CDC consumer should consume and insert to CH
    // 5. Query ClickHouse to verify

    // Expected: Event visible in ClickHouse within 2 seconds
}
```

---

## ä»£ç é›†æˆç‚¹

### ç°æœ‰æ–‡ä»¶éœ€è¦ä¿®æ”¹

#### 1. `src/main.rs`

```diff
+ // Start CDC consumer
+ tokio::spawn(async move { ... });
+
+ // Start Events consumer
+ tokio::spawn(async move { ... });
```

#### 2. `src/services/mod.rs`

```diff
+ pub mod cdc;
+ pub mod events;
```

#### 3. `Cargo.toml` (dependencies)

æ‰€æœ‰ä¾èµ–å·²åœ¨ workspace dependencies ä¸­ï¼š
- `rdkafka` âœ“
- `clickhouse` âœ“
- `sqlx` âœ“
- `redis` âœ“

---

## æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•
- CdcMessage éªŒè¯
- EventDeduplicator Redis æ“ä½œ
- Offset Manager æ•°æ®åº“æ“ä½œ

### é›†æˆæµ‹è¯•
- å®Œæ•´çš„ CDC â†’ CH ç®¡é“
- å®Œæ•´çš„ Events â†’ CH ç®¡é“
- ç«¯åˆ°ç«¯å»¶è¿Ÿæµ‹è¯•ï¼ˆ<2sï¼‰

### å‹åŠ›æµ‹è¯•
- 1000 events/sec
- 1000 CDC changes/sec
- éªŒè¯å»é‡æœ‰æ•ˆæ€§

---

## éƒ¨ç½²æ¸…å•

- [ ] PostgreSQL Debezium CDC å·²é…ç½®
- [ ] Kafka ä¸»é¢˜å·²åˆ›å»º (cdc.*, events)
- [ ] ClickHouse è¡¨å·²åˆ›å»º (posts_cdc, follows_cdc, events, etc.)
- [ ] Redis å»é‡é”®ç©ºé—´å·²å‡†å¤‡
- [ ] CDC consumer ä»£ç å®Œæˆ
- [ ] Events consumer ä»£ç å®Œæˆ
- [ ] é›†æˆæµ‹è¯•é€šè¿‡
- [ ] å‹åŠ›æµ‹è¯•é€šè¿‡ (1k RPS)
- [ ] ç›‘æ§æŒ‡æ ‡å·²é…ç½®
- [ ] è¿ç»´æ‰‹å†Œå·²å‡†å¤‡

---

## åç»­æ­¥éª¤ï¼ˆPhase 2ï¼‰

å®Œæˆ Phase 1 åï¼š

1. **ClickHouse ç‰©åŒ–è§†å›¾** (2 å¤©)
   - åˆ›å»º `post_metrics_1h` èšåˆ
   - åˆ›å»º `user_author_90d` äº²å’Œåº¦è¡¨

2. **Circuit Breaker** (1 å¤©)
   - æ·»åŠ åˆ° Feed Ranking Service
   - è‡ªåŠ¨å›é€€åˆ° PostgreSQL

3. **å®æ—¶ç¼“å­˜å¤±æ•ˆ** (1 å¤©)
   - è®¢é˜… eventsï¼Œå®æ—¶å¤±æ•ˆç”¨æˆ·ç¼“å­˜
   - å–ä»£åŸºäº TTL çš„æ–¹å¼

4. **æŒ‡æ ‡æ”¶é›†** (2 å¤©)
   - CDC lagã€Events lag
   - å»é‡ç‡ã€æ’å…¥å»¶è¿Ÿ
   - Prometheus exporters

---

## å¸¸è§é—®é¢˜

**Q: ä¸ºä»€ä¹ˆ CDC consumer ä¸åœ¨åŸæœ‰ä»£ç ä¸­ï¼Ÿ**
A: åŸæœ‰ä»£ç ä¾§é‡äº HTTP API å’Œ Redis ç¼“å­˜ã€‚CDC æ¶ˆè´¹æ˜¯æµå¤„ç†ï¼Œéœ€è¦å•ç‹¬çš„æ¶ˆè´¹è€…çº¿ç¨‹/è¿›ç¨‹ã€‚

**Q: Events consumer å’Œ existing events handler æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ**
A:
- Events handler (`src/handlers/events.rs`): HTTP ç«¯ç‚¹ï¼Œæ¥æ”¶æ¥è‡ªå®¢æˆ·ç«¯çš„äº‹ä»¶ï¼Œå†™å…¥ Kafka
- Events consumer (NEW): ä» Kafka è¯»å–äº‹ä»¶ï¼Œå†™å…¥ ClickHouse

**Q: å¦‚ä½•å¤„ç† Kafka å®•æœºï¼Ÿ**
A:
- CDC consumer ä¼šè‡ªåŠ¨é‡è¯•ï¼ˆrdkafka built-inï¼‰
- Events æš‚æ—¶ç§¯å‹åœ¨ Kafka
- æ¢å¤åè‡ªåŠ¨ç»§ç»­æ¶ˆè´¹

**Q: å»é‡æ˜¯å¦ä¼šå¯¼è‡´é‡å¤è®¡ç®—ï¼Ÿ**
A:
- Redis å»é‡æ˜¯è¾“å…¥ä¾§ï¼ˆä¸é‡å¤æ¶ˆè´¹ï¼‰
- ClickHouse ä¹Ÿæœ‰å»é‡ï¼ˆç›¸åŒ event_id åªä¿å­˜ä¸€æ¬¡ï¼‰
- ä¸¤é‡ä¿æŠ¤ç¡®ä¿æ­£ç¡®æ€§

---

## æ”¯æŒèµ„æº

- Debezium æ–‡æ¡£ï¼šhttps://debezium.io/documentation/
- rdkafka Rustï¼šhttps://docs.rs/rdkafka/
- ClickHouse å®¢æˆ·ç«¯ï¼šhttps://docs.rs/clickhouse/

---

**ä¸‹ä¸€æ­¥**: ç¡®è®¤æ˜¯å¦ä» Phase 1 å¼€å§‹å®æ–½ï¼Ÿ
